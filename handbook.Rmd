---
title: "The Hitchhiker's Guide to the `tlverse`"
subtitle: "The Targeted Learning Practitioner's Handbook"
author: "Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Alan
  Hubbard, Mark van der Laan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
documentclass: krantz
site: bookdown::bookdown_site
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
links-as-notes: false
colorlinks: yes
lot: yes
lof: yes
graphics: yes
fontsize: '11pt, krantz2'
mainfont: Palatino
monofont: "Source Code Pro"
monofontoptions: "Scale=0.8"
always_allow_html: true
description: "An open-source and fully-reproducible handbook for applying the
  targeted learning methodology in practice using the [`tlverse` software
  ecosystem](https://github.com/tlverse)."
url: 'https\://tlverse.org/tlverse-handbook/'
github-repo: tlverse/tlverse-handbook
cover-image: "img/logos/tlverse-logo.png"
favicon: "img/favicons/favicon.png"
---

```{r set-options, include=FALSE}
# Set output options
if (knitr:::is_html_output()) {
  options(width = 80)
}
if (knitr:::is_latex_output()) {
  options(width = 65)
}
options(digits = 7, bookdown.clean_book = TRUE, knitr.kable.NA = "NA")
knitr::opts_chunk$set(
  tidy = FALSE,
  out.width = "\textwidth",
  fig.align = "center",
  comment = NA
)
```

```{r pkg-bib, include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), "bookdown", "knitr", "rmarkdown"
), "packages.bib")
```

# Preface {-}

<img style="float: center; margin-right: 1%; margin-bottom: 0.01em"
     src="img/logos/tlverse-logo.svg" width="15%" height="15%">
<img style="float: center; margin-right: 1%; margin-bottom: 0.01em"
     src="img/logos/Rlogo.svg" width="15%" height="15%">
<img style="float: center; margin-right: 1%; margin-bottom: 0.01em"
     src="img/logos/vdl-logo-transparent.svg" width="15%" height="15%">
<p style="clear: both;">
<br>

## About this book {-}

*The Hitchhiker's Guide to the `tlverse`, or a Targeted Learning Practitioner's
Handbook* is an open-source and fully-reproducible electronic handbook for
applying the targeted learning methodology in practice using the [`tlverse`
software ecosystem](https://github.com/tlverse). This work is currently in an
early draft phase and is available to facilitate input from the community. To
view or contribute to the available content, consider visiting the
[GitHub repository for this site](https://github.com/tlverse/tlverse-handbook).

## Outline {#outline}

The contents of this handbook are meant to serve as a reference guide for
applied research as well as materials that can be taught in a series of short
courses focused on the applications of Targeted Learning. Each section
introduces a set of distinct causal questions, motivated by a case study,
alongside statistical methodology and software for assessing the causal claim of
interest. The (evolving) set of materials includes

* Motivation: [Why we need a statistical
    revolution](https://senseaboutscienceusa.org/super-learning-and-the-revolution-in-knowledge/)
* The Roadmap and introductory case study: the WASH Beneifits data
* Introduction to the [`tlverse` software
    ecosystem](https://tlverse.org)
* Ensemble machine learning with the
    [`sl3`](https://github.com/tlverse/sl3) package
* Targeted learning for causal inference with the
    [`tmle3`](https://github.com/tlverse/tmle3) package
* Optimal treatments regimes and the
    [`tmle3mopttx`](https://github.com/tlverse/tmle3mopttx) package
* Stochastic treatment regimes and the
    [`tmle3shift`](https://github.com/tlverse/tmle3shift) package
* _Coda_: [Why we need a statistical
    revolution](https://senseaboutscienceusa.org/super-learning-and-the-revolution-in-knowledge/)

## What this book is not {-}

The focus of this work is __not__ on providing in-depth technical descriptions
of current statistical methodology or recent advancements. Instead, the goal is
to convey key details of state-of-the-art techniques in an manner that is both
clear and complete, without burdening the reader with extraneous information.
We hope that the presentations herein will serve as references for researchers
-- methodologists and domain specialists alike -- that empower them to deploy
the central tools of Targeted Learning in an efficient manner. For technical
details and in-depth descriptions of both classical theory and recent advances
in the field of Targeted Learning, the interested reader is invited to consult
@vdl2011targeted and/or @vdl2018targeted as appropriate. The primary literature
in statistical causal inference, machine learning, and non/semiparametric theory
include many of the most recent advances in Targeted Learning and related areas.

## About the authors {-}

### Jeremy Coyle {-}

Jeremy R. Coyle, Ph.D., is a consulting data scientist and statistical
programmer, currently leading the software development effort that has produced
the `tlverse` ecosystem of R packages and related software tools. Jeremy earned
his Ph.D. in Biostatistics from UC Berkeley in 2016, primarily under the
supervision of Alan Hubbard.

### Nima Hejazi {-}

Nima S. Hejazi is a Ph.D. candidate in biostatistics with a designated emphasis
in computational and genomic biology, working jointly with Mark van der Laan and
Alan Hubbard. Nima is affiliated with UC Berkeley's Center for Computational
Biology and NIH Biomedical Big Data training program. His research interests
span causal inference, nonparametric inference and machine learning, targeted
loss-based estimation, survival analysis, statistical computing, reproducible
research, and high-dimensional biology. He is also passionate about software
development for applied statistics, including software design, automated
testing, and reproducible coding practices. For more information, see
https://nimahejazi.org.

### Ivana Malenica {-}

Ivana Malenica is a Ph.D. student in biostatistics advised by Mark van der Laan.
Ivana is currently a fellow at the Berkeley Institute for Data Science, after
serving as a NIH Biomedical Big Data and Freeport-McMoRan Genomic Engine fellow.
She earned her Master's in Biostatistics and Bachelor's in Mathematics, and
spent some time at the Translational Genomics Research Institute. Very broadly,
her research interests span non/semi-parametric theory, probability theory,
machine learning, causal inference and high-dimensional statistics. Most of her
current work involves complex dependent settings (dependence through time and
network) and adaptive sequential designs.

### Rachael Phillips {-}

Rachael is a Ph.D. student in biostatistics, advised by Alan Hubbard and Mark
van der Laan. She has an M.A. in Biostatistics, B.S. in Biology with a Chemistry
minor and a B.A. in Mathematics with a Spanish minor. Rachael's research focuses
on narrowing the gap between the theory and application of modern statistics for
real-world data science. Specifically, Rachael is motivated by issues arising in
healthcare, and she leverages strategies rooted in causal inference and
nonparametric estimation to build clinician-tailored, machine-driven solutions.
Rachael is also passionate about free, online-mediated education and its
corresponding pedagogy.

### Alan Hubbard {-}

Alan E. Hubbard is Professor of Biostatistics, former head of the Division of
Biostatistics at UC Berkeley, and head of data analytics core at UC Berkeley's
SuperFund research program. His current research interests include causal
inference, variable importance analysis, statistical machine learning,
estimation of and inference for data-adaptive statistical target parameters, and
targeted minimum loss-based estimation. Research in his group is generally
motivated by applications to problems in computational biology, epidemiology,
and precision medicine.

### Mark van der Laan {-}

Mark J. van der Laan, PhD, is Professor of Biostatistics and Statistics at UC
Berkeley. His research interests include statistical methods in computational
biology, survival analysis, censored data, adaptive designs, targeted maximum
likelihood estimation, causal inference, data-adaptive loss-based learning, and
multiple testing. His research group developed loss-based super learning in
semiparametric models, based on cross-validation, as a generic optimal tool for
the estimation of infinite-dimensional parameters, such as nonparametric density
estimation and prediction with both censored and uncensored data. Building on
this work, his research group developed targeted maximum likelihood estimation
for a target parameter of the data-generating distribution in arbitrary
semiparametric and nonparametric models, as a generic optimal methodology for
statistical and causal inference. Most recently, Mark's group has focused in
part on the development of a centralized, principled set of software tools for
targeted learning, the `tlverse`. For more information, see
https://vanderlaan-lab.org.

<!--
## Acknowledgements {-}
-->

## Learning resources {#learn}

To effectively utilize this handbook, the reader need not be a fully trained
statistician to begin understanding and applying these methods. However, it is
highly recommended for the reader to have an understanding of basic statistical
concepts such as confounding, probability distributions, confidence intervals,
hypothesis tests, and regression. Advanced knowledge of mathematical statistics
may be useful but is not necessary. Familiarity with the `R` programming
language will be essential. We also recommend an understanding of introductory
causal inference.

For learning the `R` programming language we recommend the following (free)
introductory resources:

* [Software Carpentry's _Programming with
    `R`_](http://swcarpentry.github.io/r-novice-inflammation/)
* [Software Carpentry's _`R` for Reproducible Scientific
    Analysis_](http://swcarpentry.github.io/r-novice-gapminder/)
* [Garret Grolemund and Hadley Wickham's _`R` for Data
    Science_](https://r4ds.had.co.nz)

For a general introduction to causal inference, we recommend

* [Miguel A. Hernán and James M. Robins' _Causal Inference_, forthcoming
    2020](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/)
* [Jason A. Roy's _A Crash Course in Causality: Inferring Causal Effects from
  Observational Data_ on
  Coursera](https://www.coursera.org/learn/crash-course-in-causality)

## Setup instructions {#setup}

### R and RStudio

**R** and **RStudio** are separate downloads and installations. R is the
underlying statistical computing environment. RStudio is a graphical integrated
development environment (IDE) that makes using R much easier and more
interactive. You need to install R before you install RStudio.

#### Windows

##### If you already have R and RStudio installed

* Open RStudio, and click on "Help" > "Check for updates". If a new version is
  available, quit RStudio, and download the latest version for RStudio.
* To check which version of R you are using, start RStudio and the first thing
  that appears in the console indicates the version of R you are
  running. Alternatively, you can type `sessionInfo()`, which will also display
  which version of R you are running. Go on the [CRAN
  website](https://cran.r-project.org/bin/windows/base/) and check whether a
  more recent version is available. If so, please download and install it. You
  can [check here](https://cran.r-project.org/bin/windows/base/rw-FAQ.html#How-do-I-UNinstall-R_003f)
  for more information on how to remove old versions from your system if you
  wish to do so.

##### If you don't have R and RStudio installed

* Download R from
  the [CRAN website](http://cran.r-project.org/bin/windows/base/release.htm).
* Run the `.exe` file that was just downloaded
* Go to the [RStudio download page](https://www.rstudio.com/products/rstudio/download/#download)
* Under *Installers* select **RStudio x.yy.zzz - Windows
  XP/Vista/7/8** (where x, y, and z represent version numbers)
* Double click the file to install it
* Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.

#### macOS

##### If you already have R and RStudio installed

* Open RStudio, and click on "Help" > "Check for updates". If a new version is
  available, quit RStudio, and download the latest version for RStudio.
* To check the version of R you are using, start RStudio and the first thing
  that appears on the terminal indicates the version of R you are running.
  Alternatively, you can type `sessionInfo()`, which will also display which
  version of R you are running. Go on the [CRAN
  website](https://cran.r-project.org/bin/macosx/) and check whether a more
  recent version is available. If so, please download and install it.

##### If you don't have R and RStudio installed

* Download R from
  the [CRAN website](http://cran.r-project.org/bin/macosx).
* Select the `.pkg` file for the latest R version
* Double click on the downloaded file to install R
* It is also a good idea to install [XQuartz](https://www.xquartz.org/) (needed
  by some packages)
* Go to the [RStudio download
  page](https://www.rstudio.com/products/rstudio/download/#download)
* Under *Installers* select **RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)**
  (where x, y, and z represent version numbers)
* Double click the file to install RStudio
* Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.

#### Linux

* Follow the instructions for your distribution
  from [CRAN](https://cloud.r-project.org/bin/linux), they provide information
  to get the most recent version of R for common distributions. For most
  distributions, you could use your package manager (e.g., for Debian/Ubuntu run
  `sudo apt-get install r-base`, and for Fedora `sudo yum install R`), but we
  don't recommend this approach as the versions provided by this are
  usually out of date. In any case, make sure you have at least R 3.3.1.
* Go to the [RStudio download
  page](https://www.rstudio.com/products/rstudio/download/#download)
* Under *Installers* select the version that matches your distribution, and
  install it with your preferred method (e.g., with Debian/Ubuntu `sudo dpkg -i
  rstudio-x.yy.zzz-amd64.deb` at the terminal).
* Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.

These setup instructions are adapted from those written for [Data Carpentry: R
for Data Analysis and Visualization of Ecological
Data](http://www.datacarpentry.org/R-ecology-lesson/).

<!--chapter:end:index.Rmd-->

# Motivation {-}

> "One enemy of robust science is our humanity — our appetite for
> being right, and our tendency to find patterns in noise, to see supporting
> evidence for what we already believe is true, and to ignore the facts that do
> not fit."
>
> --- @naturenews_2015

Scientific research is at a unique point in history. The need to improve rigor
and reproducibility in our field is greater than ever; corroboration moves
science forward, yet there is a growing alarm about results that cannot be
reproduced and that report false discoveries [@baker2016there]. Consequences of
not meeting this need will result in further decline in the rate of scientific
progression, the reputation of the sciences, and the public’s trust in its
findings [@munafo2017manifesto; @naturenews2_2015].

> "The key question we want to answer when seeing the results of any scientific
> study is whether we can trust the data analysis."
>
> --- @peng2015reproducibility

Unfortunately, at its current state the culture of data analysis and statistics
actually enables human bias through improper model selection. All hypothesis
tests and estimators are derived from statistical models, so to obtain valid
estimates and inference it is critical that the statistical model contains the
process that generated the data. Perhaps treatment was randomized or only
depended on a small number of baseline covariates; this knowledge should and
can be incorporated in the model. Alternatively, maybe the data is
observational, and there is no knowledge about the data-generating process (DGP).
If this is the case, then the statistical model should contain *all* data
distributions. In practice; however, models are not selected based on knowledge
of the DGP, instead models are often selected based on (1) the p-values they
yield, (2) their convenience of implementation, and/or (3) an analysts loyalty
to a particular model. This practice of "cargo-cult statistics --- the
ritualistic miming of statistics rather than conscientious practice,"
[@stark2018cargo] is characterized by arbitrary modeling choices, even though
these choices often result in different answers to the same research question.
That is, "increasingly often, [statistics] is used instead to aid and
abet weak science, a role it can perform well when used mechanically or
ritually," as opposed to its original purpose of safeguarding against weak
science [@stark2018cargo]. This presents a fundamental drive behind the epidemic
of false findings that scientific research is suffering from [@vdl2014entering].

> "We suggest that the weak statistical understanding is probably due to
> inadequate "statistics lite" education. This approach does not build up
> appropriate mathematical fundamentals and does not provide scientifically
> rigorous introduction into statistics. Hence, students' knowledge may remain
> imprecise, patchy, and prone to serious misunderstandings. What this approach
> achieves, however, is providing students with false confidence of being able
> to use inferential tools whereas they usually only interpret the p-value
> provided by black box statistical software. While this educational problem
> remains unaddressed, poor statistical practices will prevail regardless of
> what procedures and measures may be favored and/or banned by editorials."
>
> --- @szucs2017null


Our team at The University of California, Berkeley, is uniquely positioned to
provide such an education. Spearheaded by Professor Mark van der Laan, and
spreading rapidly by many of his students and colleagues who have greatly
enriched the field, the aptly named "Targeted Learning" methodology targets the
scientific question at hand and is counter to the current culture of
"convenience statistics" which opens the door to biased estimation, misleading
results, and false discoveries. Targeted Learning restores the fundamentals that
formalized the field of statistics, such as the that facts that a statistical
model represents real knowledge about the experiment that generated the data,
and a target parameter represents what we are seeking to learn from the data as
a feature of the distribution that generated it [@vdl2014entering]. In this way,
Targeted Learning defines a truth and establishes a principled standard for
estimation, thereby inhibiting these all-too-human biases (e.g., hindsight bias,
confirmation bias, and outcome bias) from infiltrating analysis.

> "The key for effective classical [statistical] inference is to have
> well-defined questions and an analysis plan that tests those questions."
>
> --- @nosek2018preregistration

The objective for this handbook is to provide training to students, researchers,
industry professionals, faculty in science, public health, statistics, and other
fields to empower them with the necessary knowledge and skills to utilize the
sound methodology of Targeted Learning --- a technique that provides tailored
pre-specified machines for answering queries, so that each data analysis is
completely reproducible, and estimators are efficient, minimally biased, and
provide formal statistical inference.

Just as the conscientious use of modern statistical methodology is necessary to
ensure that scientific practice thrives, it remains critical to acknowledge the
role that robust software plays in allowing practitioners direct access to
published results. We recall that "an article...in a scientific publication is
not the scholarship itself, it is merely advertising of the scholarship. The
actual scholarship is the complete software development environment and the
complete set of instructions which generated the figures," thus making the
availability and adoption of robust statistical software key to enhancing the
transparency that is an inherent aspect of science [@buckheit1995wavelab].

For a statistical methodology to be readily accessible in practice, it is
crucial that it is accompanied by robust user-friendly software
[@pullenayegum2016knowledge; @stromberg2004write]. The `tlverse` software
ecosystem was developed to fulfill this need for the Targeted Learning
methodology. Not only does this software facilitate computationally reproducible
and efficient analyses, it is also a tool for Targeted Learning education since
its workflow mirrors that of the methodology. In particular, the `tlverse`
paradigm does not focus on implementing a specific estimator or a small set of
related estimators. Instead, the focus is on exposing the statistical framework
of Targeted Learning itself --- all `R` packages in the `tlverse` ecosystem
directly model the key objects defined in the mathematical and theoretical
framework of Targeted Learning. What's more, the `tlverse` `R` packages share a
core set of design principles centered on extensibility, allowing for them to be
used in conjunction with each other and built upon one other in a cohesive
fashion.

In this handbook, the reader will embark on a journey through the `tlverse`
ecosystem. Guided by `R` programming exercises, case studies, and
intuitive explanation readers will build a toolbox for applying the Targeted
Learning statistical methodology, which will translate to real-world causal
inference analyses. Some preliminaries are required prior to this learning
endeavor -- we have made available a list of [recommended learning
resources](#learn).

<!--chapter:end:01-preface.Rmd-->

# The Roadmap for Targeted Learning {#intro}

## Learning Objectives

By the end of this chapter you will be able to:

1. Translate scientific questions to statistical questions.
2. Define a statistical model based on the knowledge of the experiment that
   generated the data.
3. Identify a causal parameter as a function of the observed data distribution.
4. Explain the following causal and statistical assumptions and their
   implications: i.i.d., consistency, interference, positivity, SUTVA.

## Introduction

The roadmap of statistical learning is concerned with the translation from
real-world data applications to a mathematical and statistical formulation of
the relevant estimation problem. This involves data as a random variable having
a probability distribution, scientific knowledge represented by a statistical
model, a statistical target parameter representing an answer to the question of
interest, and the notion of an estimator and sampling distribution of the
estimator.

## The Roadmap

Following the roadmap is a process of five stages.

1. Data as a random variable with a probability distribution, $O \sim P_0$.
2. The statistical model $\mathcal{M}$ such that $P_0 \in \mathcal{M}$.
3. The statistical target parameter $\Psi$ and estimand $\Psi(P_0)$.
4. The estimator $\hat{\Psi}$ and estimate $\hat{\Psi}(P_n)$.
5. A measure of uncertainty for the estimate $\hat{\Psi}(P_n)$.

### (1) Data: A random variable with a probability distribution, $O \sim P_0$ {-}

The data set we're confronted with is the result of an experiment and we can
view the data as a random variable, $O$, because if we repeat the experiment
we would have a different realization of this experiment. In particular, if we
repeat the experiment many times we could learn the probability distribution,
$P_0$, of our data. So, the observed data $O$ with probability distribution
$P_0$ are $n$ independent identically distributed (i.i.d.) observations of the
random variable $O; O_1, \ldots, O_n$. Note that while not all data are i.i.d.,
there are ways to handle non-i.i.d. data, such as establishing conditional
independence, stratifying data to create sets of identically distributed data,
etc. It is crucial that researchers be absolutely clear about what they actually
know about the data-generating distribution for a given problem of interest.
Unfortunately, communication between statisticians and researchers is often
fraught with misinterpretation. The roadmap provides a mechanism by which to
ensure clear communication between research and statistician -- it truly helps
with this communication!

#### The empirical probability measure, $P_n$ {-}

Once we have $n$ of such i.i.d. observations we have an empirical probability
measure, $P_n$. The empirical probability measure is an approximation of the
true probability measure $P_0$, allowing us to learn from our data. For
example, we can define the empirical probability measure of a set, $A$, to be
the proportion of observations which end up in $A$. That is,
\begin{equation*}
  P_n(A) = \frac{1}{n}\sum_{i=1}^{n} \mathbb{I}(O_i \in A)
\end{equation*}

In order to start learning something, we need to ask *"What do we know about the
probability distribution of the data?"* This brings us to Step 2.

### (2) The statistical model $\mathcal{M}$ such that $P_0 \in \mathcal{M}$ {-}

The statistical model $\mathcal{M}$ is defined by the question we asked at the
end of \ref{step1}. It is defined as the set of possible probability
distributions for our observed data. Often $\mathcal{M}$ is very large (possibly
infinite-dimensional), to reflect the fact that statistical knowledge is
limited. In the case that $\mathcal{M}$ is infinite-dimensional, we deem this a
nonparametric statistical model.

Alternatively, if the probability distribution of the data at hand is described
by a finite number of parameters, then the statistical model is parametric. In
this case, we prescribe to the belief that the random variable $O$ being
observed has, e.g., a normal distribution with mean $\mu$ and variance
$\sigma^2$. More formally, a parametric model may be defined
\begin{equation*}
  \mathcal{M} = \{P_{\theta} : \theta \in \mathcal{R}^d \}
\end{equation*}

Sadly, the assumption that the data-generating distribution has a specific,
parametric forms is all-too-common, even when such is a leap of faith. This
practice of oversimplification in the current culture of data analysis typically
derails any attempt at trying to answer the scientific question at hand; alas,
such statements as the ever-popular quip of Box that "All models are wrong but
some are useful," encourage the data analyst to make arbitrary choices even when
that often force significant differences in answers to the same estimation
problem. The Targeted Learning paradigm does not suffer from this bias since it
defines the statistical model through a representation of the true
data-generating distribution corresponding to the observed data.

Now, on to Step 3: *``What are we trying to learn from the data?"*

### (3) The statistical target parameter $\Psi$ and estimand $\Psi(P_0)$ {-}

The statistical target parameter, $\Psi$, is defined as a mapping from the
statistical model, $\mathcal{M}$, to the parameter space (i.e., a real number)
$\mathcal{R}$. That is, $\Psi: \mathcal{M}\rightarrow\mathbb{R}$. The estimand
may be seen as a representation of the quantity that we wish to learn from the
data, the answer to a well-specified (often causal) question of interest. In
contrast to purely statistical estimands, causal estimands require
_identification from the observed data_, based on causal models that include
several untestable assumptions, described in more detail in the section on
[causal target parameters](#causal).

For a simple example, consider a data set which contains observations of a
survival time on every subject, for which our question of interest is "What's
the probability that someone lives longer than five years?" We have,
\begin{equation*}
  \Psi(P_0) = \mathbb{P}(O > 5)
\end{equation*}

This answer to this question is the **estimand, $\Psi(P_0)$**, which is the
quantity we're trying to learn from the data. Once we have defined $O$,
$\mathcal{M}$ and $\Psi(P_0)$ we have formally defined the statistical
estimation problem.

### (4) The estimator $\hat{\Psi}$ and estimate $\hat{\Psi}(P_n)$ {-}

To obtain a good approximation of the estimand, we need an estimator, an _a
priori_-specified algorithm defined as a mapping from the set of possible
empirical distributions, $P_n$, which live in a non-parametric statistical
model, $\mathcal{M}_{NP}$ ($P_n \in \mathcal{M}_{NP}$), to the parameter space
of the parameter of interest. That is, $\hat{\Psi} : \mathcal{M}_{NP} \rightarrow
\mathbb{R}^d$. The estimator is a function that takes as input
the observed data, a realization of $P_n$, and gives as output a value in the
parameter space, which is the **estimate, $\hat{\Psi}(P_n)$**.

Where the estimator may be seen as an operator that maps the observed data and
corresponding empirical distribution to a value in the parameter space, the
numerical output that produced such a function is the estimate. Thus, it is an
element of the parameter space based on the empirical probability distribution
of the observed data. If we plug in a realization of $P_n$ (based on a sample
size $n$ of the random variable $O$), we get back an estimate $\hat{\Psi}(P_n)$
of the true parameter value $\Psi(P_0)$.

In order to quantify the uncertainty in our estimate of the target parameter
(i.e., to construct statistical inference), an understanding of the sampling
distribution of our estimator will be necessary. This brings us to Step 5.

### (5) A measure of uncertainty for the estimate $\hat{\Psi}(P_n)$ {-}

Since the estimator $\hat{\Psi}$ is a function of the empirical
distribution $P_n$, the estimator itself is a random variable with a sampling
distribution. So, if we repeat the experiment of drawing $n$ observations we
would every time end up with a different realization of our estimate and our
estimator has a sampling distribution. The sampling distribution of an estimator
can be theoretically validated to be approximately normally distributed by a
Central Limit Theorem (CLT).

A class of __Central Limit Theorems__ (CLTs) are statements regarding the
convergence of the __sampling distribution of an estimator__ to a normal
distribution. In general, we will construct estimators whose limit sampling
distributions may be shown to be approximately normal distributed as sample size
increases. For large enough $n$ we have,
\begin{equation*}
  \hat{\Psi}(P_n) \sim N \left(\Psi(P_0), \frac{\sigma^2}{n}\right),
\end{equation*}
permitting statistical inference. Now, we can proceed to quantify the
uncertainty of our chosen estimator by construction of hypothesis tests and
confidence intervals. For example, we may construct a confidence interval at
level $(1 - \alpha)$ for our estimand, $\Psi(P_0)$:
\begin{equation*}
  \hat{\Psi}(P_n) \pm z_{1 - \frac{\alpha}{2}}
    \left(\frac{\sigma}{\sqrt{n}}\right),
\end{equation*}
where $z_{1 - \frac{\alpha}{2}}$ is the $(1 - \frac{\alpha}{2})^\text{th}$
quantile of the standard normal distribution. Often, we will be interested in
constructing 95% confidence intervals, corresponding to mass $\alpha = 0.05$ in
either tail of the limit distribution; thus, we will typically take
$z_{1 - \frac{\alpha}{2}} \approx 1.96$.

_Note:_ we will typically have to estimate the standard error,
$\frac{\sigma}{\sqrt{n}}$.

A 95\% confidence interval means that if we were to take 100 different samples
of size $n$ and compute a 95% confidence interval for each sample then
approximately 95 of the 100 confidence intervals would contain the estimand,
$\Psi(P_0)$. More practically, this means that there is a 95% probability
(or 95% confidence) that the confidence interval procedure will contain the
true estimand. However, any single estimated confidence interval either will
contain the true estimand or will not.

## Summary of the Roadmap

Data, $O$, is viewed as a random variable that has a probability distribution.
We often have $n$ units of independent identically distributed units with
probability distribution $P_0$ ($O_1, \ldots, O_n \sim P_0$). We have
statistical knowledge about the experiment that generated this data. In other
words, we make a statement that the true data distribution $P_0$ falls in a
certain set called a statistical model, $\mathcal{M}$. Often these sets are very
large because statistical knowledge is very limited so these statistical models
are often infinite dimensional models. Our statistical query is, "What are we
trying to learn from the data?" denoted by the statistical target parameter,
$\Psi$, which maps the $P_0$ into the estimand, $\Psi(P_0)$. At this point the
statistical estimation problem is formally defined and now we will need
statistical theory to guide us in the construction of estimators. There's a lot
of statistical theory we will review in this course that, in particular, relies
on the Central Limit Theorem, allowing us to come up with estimators that are
approximately normally distributed and also allowing us to come with statistical
inference (i.e., confidence intervals and hypothesis tests).

## Causal Target Parameters {#causal}

In many cases, we are interested in questions that ask questions regarding the
effect of an intervention on a future outcome of interest. These questions can
be represented as causal estimands and

### The Causal Model

After formalizing the data and the statistical model, we can define a causal
model to express causal parameters of interest. Directed acyclic graphs (DAGs)
are one useful tool to express what we know about the causal relations among
variables. Ignoring exogenous $U$ terms (explained below), we assume the
following ordering of the variables in the observed data $O$.

```{r, echo=FALSE, eval=TRUE}
library(visNetwork)
nodes <- data.frame(id = c("W", "A", "Y"))
nodes$label <- nodes$id
edges <- data.frame(from = c("W", "W", "A"), to = c("A", "Y", "Y"))
network <- visNetwork(nodes, edges, height = "300px", width = "200px") %>%
  visEdges(arrows = list(to = TRUE)) %>%
  visLayout(randomSeed = 25)
network
```

While directed acyclic graphs (DAGs) like above provide a convenient means by
which to visualize causal relations between variables, the same causal relations
among variables can be represented via a set of structural equations, which
define the non-parametric structural equation model (NPSEM):
\begin{align*}
  W &= f_W(U_W) \\
  A &= f_A(W, U_A) \\
  Y &= f_Y(W, A, U_Y),
\end{align*}
where $U_W$, $U_A$, and $U_Y$ represent the unmeasured exogenous background
characteristics that influence the value of each variable. In the NPSEM, $f_W$,
$f_A$ and $f_Y$ denote that each variable (for $W$, $A$ and $Y$, respectively)
is a function of its parents and unmeasured background characteristics, but note
that there is no imposition of any particular functional constraints(e.g.,
linear, logit-linear, only one interaction, etc.). For this reason, they are
called non-parametric structural equation models (NPSEMs). The DAG and set of
nonparametric structural equations represent exactly the same information and so
may be used interchangeably.

The first hypothetical experiment we will consider is assigning exposure to the
whole population and observing the outcome, and then assigning no exposure to
the whole population and observing the outcome. On the nonparametric structural
equations, this corresponds to a comparison of the outcome distribution in the
population under two interventions:

1. $A$ is set to $1$ for all individuals, and
2. $A$ is set to $0$ for all individuals.

These interventions imply two new nonparametric structural equation models. For
the case $A = 1$, we have
\begin{align*}
  W &= f_W(U_W) \\
  A &= 1 \\
  Y(1) &= f_Y(W, 1, U_Y),
\end{align*}
and for the case $A=0$,
\begin{align*}
  W &= f_W(U_W) \\
  A &= 0 \\
  Y(0) &= f_Y(W, 0, U_Y).
\end{align*}

In these equations, $A$ is no longer a function of $W$ because we have
intervened on the system, setting $A$ deterministically to either of the values
$1$ or $0$. The new symbols $Y(1)$ and $Y(0)$ indicate the outcome variable in
our population if it were generated by the respective NPSEMs above; these are
often called _counterfactuals_ (since they run contrary-to-fact). The difference
between the means of the outcome under these two interventions defines a
parameter that is often called the "average treatment effect" (ATE), denoted
\begin{equation}\label{eqn:ate}
  ATE = \mathbb{E}_X(Y(1)-Y(0)),
\end{equation}
where $\mathbb{E}_X$ is the mean under the theoretical (unobserved) full data
$X = (W, Y(1), Y(0))$.

Note, we can define much more complicated interventions on NPSEM's, such as
interventions based upon rules (themselves based upon covariates), stochastic
rules, etc. and each results in a different targeted parameter and entails
different identifiability assumptions discussed below.

### Identifiability

Because we can never observe both $Y(0)$ (the counterfactual outcome when $A=0$)
and $Y(1)$ (similarly, the counterfactual outcome when $A=1$), we cannot
estimate \ref{eqn:ate} directly. Instead, we have to make assumptions under
which this quantity may be estimated from the observed data $O \sim P_0$ under
the data-generating distribution $P_0$. Fortunately, given the causal model
specified in the NPSEM above, we can, with a handful of untestable assumptions,
estimate the ATE, even from observational data. These assumptions may be
summarized as follows

1. The causal graph implies $Y(a) \perp A$ for all $a \in \mathcal{A}$, which
   is the _randomization_ assumption. In the case of observational data, the
   analogous assumption is _strong ignorability_ or _no unmeasured confounding_
   $Y(a) \perp A \mid W$ for all $a \in \mathcal{A}$;
2. Although not represented in the causal graph, also required is the assumption
   of no interference between units, that is, the outcome for unit $i$ $Y_i$ is
   not affected by exposure for unit $j$ $A_j$ unless $i=j$;
3. _Consistency_ of the treatment mechanism is also required, i.e., the outcome
   for unit $i$ is $Y_i(a)$ whenever $A_i = a$, an assumption also known as "no
   other versions of treatment";
4. It is also necessary that all observed units, across strata defined by $W$,
   have a bounded (non-deterministic) probability of receiving treatment --
   that is, $0 < \mathbb{P}(A = a \mid W) < 1$ for all $a$ and $W$). This assumption
   is referred to as _positivity_ or _overlap_.

_Remark_: Together, (2) and (3), the assumptions of no interference and
consistency, respectively, are jointly referred to as the *stable unit
treatment value assumption* (SUTVA).

Given these assumptions, the ATE may be re-written as a function of $P_0$,
specifically
\begin{equation}\label{eqn:estimand}
  ATE = \mathbb{E}_0(Y(1) - Y(0)) = \mathbb{E}_0
    \left(\mathbb{E}_0[Y \mid A = 1, W] - \mathbb{E}_0[Y \mid A = 0, W]\right),
\end{equation}
or the difference in the predicted outcome values for each subject, under the
contrast of treatment conditions ($A = 0$ vs. $A = 1$), in the population,
averaged over all observations. Thus, a parameter of a theoretical "full" data
distribution can be represented as an estimand of the observed data
distribution. Significantly, there is nothing about the representation in
\ref{eqn:estimand} that requires parameteric assumptions; thus, the regressions
on the right hand side may be estimated freely with machine learning. With
different parameters, there will be potentially different identifiability
assumptions and the resulting estimands can be functions of different components
of $P_0$. We discuss several more complex estimands in later sections of this
handbook.

<!--chapter:end:02-intro.Rmd-->

# Welcome to the `tlverse` {#tlverse}

## Learning Objectives

1. Understand the `tlverse` ecosystem conceptually
2. Identify the core components of the `tlverse`
3. Install `tlverse` `R` packages
4. Understand the Targeted Learning roadmap
5. Learn about the WASH Benefits example data

## What is the `tlverse`?

The `tlverse` is a new framework for doing Targeted Learning in R, inspired by
the [`tidyverse` ecosystem](https://tidyverse.org) of R packages.

By analogy to the [`tidyverse`](https://tidyverse.org/):

> The `tidyverse` is an opinionated collection of R packages designed for data
> science. All packages share an underlying design philosophy, grammar, and data
> structures.

So, the [`tlverse`](https://tlverse.org) is

* an opinionated collection of R packages for Targeted Learning
* sharing an underlying philosophy, grammar, and set of data structures

## `tlverse` components

These are the main packages that represent the **core** of the `tlverse`:

* [`sl3`](https://github.com/tlverse/sl3): Modern Super Learning with Pipelines
  * _What?_ A modern object-oriented re-implementation of the Super Learner
    algorithm, employing recently developed paradigms for `R` programming.
  * _Why?_ A design that leverages modern tools for fast computation, is
    forward-looking, and can form one of the cornerstones of the `tlverse`.

* [`tmle3`](https://github.com/tlverse/tmle3): An Engine for Targeted Learning
  * _What?_ A generalized framework that simplifies Targeted Learning by
    identifying and implementing a series of common statistical estimation
    procedures.
  * _Why?_ A common interface and engine that accommodates current algorithmic
    approaches to Targeted Learning and is still flexible enough to remain the
    engine even as new techniques are developed.

In addition to the engines that drive development in the `tlverse`, there are
some supporting packages -- in particular, we have two...

* [`origami`](https://github.com/tlverse/origami): A Generalized Framework for
   Cross-Validation
  * _What?_ A generalized framework for flexible cross-validation
  * _Why?_ Cross-validation is a key part of ensuring error estimates are honest
    and preventing overfitting. It is an essential part of the both the Super
    Learner algorithm and Targeted Learning.

* [`delayed`](https://github.com/tlverse/delayed): Parallelization Framework for
   Dependent Tasks
  * _What?_ A framework for delayed computations (futures) based on task
    dependencies.
  * _Why?_ Efficient allocation of compute resources is essential when deploying
    large-scale, computationally intensive algorithms.

A key principle of the `tlverse` is extensibility. That is, we want to support
new Targeted Learning estimators as they are developed. The model for this is
new estimators are implemented in additional packages using the core packages
above. There are currently two featured examples of this:

* [`tmle3mopttx`](https://github.com/tlverse/tmle3mopttx): Optimal Treatments
  in `tlverse`
  * _What?_ Learn an optimal rule and estimate the mean outcome under the rule
  * _Why?_ Optimal Treatment is a powerful tool in precision healthcare and
    other settings where a one-size-fits-all treatment approach is not
    appropriate.

* [`tmle3shift`](https://github.com/tlverse/tmle3shift): Shift Interventions in
  `tlverse`
  * _What?_ Shift interventions for continuous treatments
  * _Why?_ Not all treatment variables are discrete. Being able to estimate the
    effects of continuous treatment represents a powerful extension of the
    Targeted Learning approach.

## Installation {#installtlverse}

The `tlverse` ecosystem of packages are currently hosted at
https://github.com/tlverse, not yet on [CRAN](http://cran.r-project.org/). You
can use the `devtools` package to install them:

```{r installation, eval=FALSE}
install.packages("devtools")
devtools::install_github("tlverse/tlverse")
```

The `tlverse` depends on a large number of other packages that are also hosted
on GitHub. Because of this, you may see the following error:

```
Error: HTTP error 403.
  API rate limit exceeded for 71.204.135.82. (But here's the good news:
  Authenticated requests get a higher rate limit. Check out the documentation
  for more details.)

  Rate limit remaining: 0/60
  Rate limit reset at: 2019-03-04 19:39:05 UTC

  To increase your GitHub API rate limit
  - Use `usethis::browse_github_pat()` to create a Personal Access Token.
  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`.
```

This just means that R tried to install too many packages from GitHub in too
short of a window. To fix this, you need to tell R how to use GitHub as your
user (you'll need a GitHub user account). Follow these two steps:

1. Type `usethis::browse_github_pat()` in your R console, which will direct
   you to GitHub's page to create a New Personal Access Token.
2. Create a Personal Access Token simply by clicking "Generate token" at the
   bottom of the page.
3. Copy your Personal Access Token, a long string of lowercase letters and
   numbers.
4. Type `usethis::edit_r_environ()` in your R console, which will open your
   `.Renviron` file in the source window of RStudio. If you are not able to 
   access your `.Renviron` file with this command, then try inputting
   `Sys.setenv(GITHUB_PAT = )` with your Personal Access Token inserted as a 
   string after the equals symbol; and if this does not error, then skip to 
   step 8. 
5. In your `.Renviron` file, type `GITHUB_PAT=` and then paste your Personal
   Access Token after the equals symbol with no space.
6. In your `.Renviron` file, press the enter key to ensure that your `.Renviron`
   ends with a newline.
7. Save your `.Renviron` file.
8. Restart R for changes to take effect. You can restart R via the drop-down
   menu on the "Session" tab. The "Session" tab is at the top of the RStudio
   interface.

After following these steps, you should be able to successfully install the
package which threw the error above.

<!--chapter:end:03-tlverse.Rmd-->

# Datasets {#data}

## WASH Benefits Example Dataset {#wash}

The data come from a study of the effect of water quality, sanitation, hand
washing, and nutritional interventions on child development in rural Bangladesh
(WASH Benefits Bangladesh): a cluster randomized controlled trial
[@luby2018effects]. The study enrolled pregnant women in their first or second
trimester from the rural villages of Gazipur, Kishoreganj, Mymensingh, and
Tangail districts of central Bangladesh, with an average of eight women per
cluster. Groups of eight geographically adjacent clusters were block randomized,
using a random number generator, into six intervention groups (all of which
received weekly visits from a community health promoter for the first 6 months
and every 2 weeks for the next 18 months) and a double-sized control group (no
intervention or health promoter visit). The six intervention groups were:

1. chlorinated drinking water;
2. improved sanitation;
3. hand-washing with soap;
4. combined water, sanitation, and hand washing;
5. improved nutrition through counseling and provision of lipid-based nutrient
   supplements; and
6. combined water, sanitation, handwashing, and nutrition.

In the handbook, we concentrate on child growth (size for age) as the outcome of
interest. For reference, this trial was registered with ClinicalTrials.gov as
NCT01590095.

```{r load_washb_data_intro, message=FALSE, warning=FALSE}
library(tidyverse)
# read in data
dat <- read_csv("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv")
dat
```
For the purposes of this handbook, we start by treating the data as independent and
identically distributed (i.i.d.) random draws from a very large target
population. We could, with available options, account for the clustering of the
data (within sampled geographic units), but, for simplification, we avoid these
details in the handbook, although modifications of our methodology for biased
samples, repeated measures, etc., are available.

We have 28 variables measured, of which 1 variable is set to be the outcome of
interest. This outcome, $Y$, is the weight-for-height Z-score (`whz` in `dat`);
the treatment of interest, $A$, is the randomized treatment group (`tr` in
`dat`); and the adjustment set, $W$, consists simply of *everything else*. This
results in our observed data structure being $n$ i.i.d. copies of $O_i = (W_i,
A_i, Y_i)$, for $i = 1, \ldots, n$.

Using the [`skimr` package](https://CRAN.R-project.org/package=skimr), we can
quickly summarize the variables measured in the WASH Benefits data set:

```{r skim_washb_data, eval=FALSE, message=FALSE, warning=FALSE}
library(skimr)
skim(dat)
```
A convenient summary of the relevant variables is given just above, complete
with a small visualization describing the marginal characteristics of each
covariate. Note that the *asset* variables reflect socio-economic status of the
study participants. Notice also the uniform distribution of the treatment groups
(with twice as many controls); this is, of course, by design.

## International Stroke Trial Example Dataset {#ist}

The International Stroke Trial database contains individual patient data from
the International Stroke Trial (IST), a multi-national randomized trial
conducted between 1991 and 1996 (pilot phase between 1991 and 1993) that aimed
to assess whether early administration of aspirin, heparin, both aspirin and
heparin, or neither influenced the clinical course of acute ischaemic stroke
[@sandercock1997international]. The IST dataset includes data on 19,435 patients
with acute stroke, with 99\% complete follow-up. De-identified data are
available for download at https://datashare.is.ed.ac.uk/handle/10283/128. This
study is described in more detail in @sandercock2011international. The example
data for this handbook considers a sample of 5,000 patients and the binary
outcome of recurrent ischemic stroke within 14 days after randomization. Also
in this example data, we ensure that we have subjects with a missing outcome.

```{r load_ist_data_intro, message=FALSE, warning=FALSE}
library(tidyverse)
# read in data
ist <- read_csv("https://raw.githubusercontent.com/tlverse/tlverse-handbook/master/data/ist_sample.csv")
ist
```

We have 26 variables measured, and the outcome of interest, $Y$, indicates recurrent
ischemic stroke within 14 days after randomization (`DRSISC` in
`ist`); the treatment of interest, $A$, is the randomized aspirin vs. no aspirin
treatment allocation (`RXASP` in `ist`); and the adjustment set, $W$, consists
of all other variables measured at baseline. In this data, the outcome is
occasionally missing, but there is no need to create a variable indicating this
missingness (such as $\Delta$) for analyses in the `tlverse`, since it is
automatically detected when `NA` are present in the outcome. This observed data
structure can be denoted as $n$ i.i.d. copies of $O_i = (W_i, A_i, \Delta_i,
\Delta Y_i)$, for $i = 1, \ldots, n$, where $\Delta$ denotes the binary
indicator that the outcome is observed.

Like before, we can summarize the variables measured in the IST sample data set
with `skimr`:

```{r skim_ist_data, eval=FALSE, message=FALSE, warning=FALSE}
skim(ist)
```

## Veterans’ Administration Lung Cancer Trial Dataset {#vet}

This data corresponds to a study conducted by the US Veterans Administration.
Male patients with advanced inoperable lung cancer were given either the
standard therapy or a test chemotherapy. The primary goal of the study was to
assess if the test chemotherapy improved survival. This data set has been
published in @kalbfleisch2011statistical and it is available in the `MASS` and
`survival` `R` packages. Time-to-death was recorded for 128 patients, and 9
patients left the study before death. Various covariates were also documented
for each patient.

```{r load_vet_data_intro, message=FALSE, warning=FALSE}
library(tidyverse)
# read in data
vet <- read_csv("https://raw.githubusercontent.com/tlverse/tlverse-handbook/master/data/veteran.csv")
vet
```

A snapshot of the data set in shown below:

```{r skim_vet_data, eval=FALSE, message=FALSE, warning=FALSE}
skim(vet)
```

<!--chapter:end:04-data.Rmd-->

# Super (Machine) Learning {#sl3}

_Rachael Phillips_

Based on the [`sl3` `R` package](https://github.com/tlverse/sl3) by _Jeremy
Coyle, Nima Hejazi, Ivana Malenica, and Oleg Sofrygin_.

Updated: `r Sys.Date()`

## Learning Objectives
By the end of this chapter you will be able to:

1. Select a loss function that is appropriate for the functional parameter to be
   estimated.
2. Assemble an ensemble of learners based on the properties that identify what
   features they support.
3. Customize learner hyperparameters to incorporate a diversity of different
   settings.
4. Select a subset of available covariates and pass only those variables to the
   modeling algorithm.
5. Fit an ensemble with nested cross-validation to obtain an estimate of the
   performance of the ensemble itself.
6. Obtain `sl3` variable importance metrics.
7. Interpret the discrete and continuous Super Learner fits.
8. Rationalize the need to remove bias from the Super Learner to make an optimal
   bias–variance tradeoff for the parameter of interest.

## Motivation

- A common task in statistical data analysis is estimator selection (e.g., for 
  prediction).
- There is no universally optimal machine learning algorithm for density 
  estimation or prediction.
- For some data, one needs learners that can model a complex function.
- For others, possibly as a result of noise or insufficient sample size, a 
  simple, parametric model might fit best.
- The Super Learner, an ensemble learner, solves this issue, by allowing a
  combination of learners from the simplest (intercept-only) to most complex
  (neural nets, random forests, SVM, etc).
- It works by using cross-validation in a manner which guarantees that the
  resulting fit will be as good as possible, given the learners provided. 

## Introduction

In [Chapter 1](#intro), we introduced the Roadmap for Targeted Learning as a
general template to translate real-world data applications into formal
statistical estimation problems. The first steps of this roadmap define the
*statistical estimation problem*, which establish

1. Data as a realization of a random variable, or equivalently, an outcome of a
   particular experiment.
2. A statistical model, representing the true knowledge about the
   data-generating experiment.
3. A translation of the scientific question, which is often causal, into a
   target parameter.

Note that if the target parameter is causal, step 3 also requires
establishing identifiability of the target quantity from the observed data
distribution, under possible non-testable assumptions that may not necessarily
be reasonable. Still, the target quantity does have a valid statistical
interpretation. See [causal target parameters](#causal) for more detail on
causal models and identifiability.

Now that we have defined the statistical estimation problem, we are ready to
construct the TMLE; an asymptotically linear and efficient substitution
estimator of this target quantity. The first step in this estimation procedure
is an initial estimate of the data-generating distribution, or the relevant part
of this distribution that is needed to evaluate the target parameter. For this
initial estimation, we use the Super Learner [@vdl2007super]. 

The Super Learner provides an important step in creating a robust estimator. It 
is a loss-function-based tool that uses cross-validation to obtain the best
prediction of our target parameter, based on a weighted average of a library of
machine learning algorithms. 

The library of machine learning algorithms consists of functions ("learners" in
the `sl3` nomenclature) that we think might be consistent with the true 
data-generating distribution (i.e. algorithms selected based on contextual 
knowledge of the experiment that generated the data). Also, the library should  
contain a large set of "default" algorithms that may range from a simple linear 
regression model to multi-step algorithms involving screening covariates, 
penalizations, optimizing tuning parameters, etc. 

The ensembling of the collection of algorithms with weights ("metalearning" in 
the `sl3` nomenclature) has been shown to be adaptive and robust, even in small 
samples [@polley2010super]. The Super Learner is proven to be asymptotically as 
accurate as the best possible prediction algorithm in the library 
[@vdl2003unified; @van2006oracle].

### Background

A *loss function* $L$ is defined as a function of the observed data and a
candidate parameter value $\psi$, which has unknown true value $\psi_0$,
$L(\psi)(O)$. We can estimate the loss by substituting the empirical
distribution $P_n$ for the true (but unknown) distribution of the observed data
$P_0$. A valid loss function will have expectation (risk) that is minimized at
the true value of the parameter $\psi_0$. For example, the conditional mean
minimizes the risk of the squared error loss. Thus, it is a valid loss function
when estimating the conditional mean.

The *discrete Super Learner*, or *cross-validation selector*, is the algorithm
in the library that minimizes the cross-validated empirical risk. 

The *cross-validated empirical risk* of an algorithm is defined as the empirical 
mean over a validation sample of the loss of the algorithm fitted on the 
training sample, averaged across the splits of the data.

The *continuous/ensemble Super Learner*, often referred to as *Super Learner* 
is a weighted average of the library of algorithms, where the weights are chosen 
to minimize the cross-validated empirical risk of the library. Restricting the 
weights to be positive and sum to one (i.e., a convex combination) has been 
shown to improve upon the discrete Super Learner [@polley2010super; 
@vdl2007super]. This notion of weighted combinations was introduced in 
@wolpert1992stacked for neural networks and adapted for regressions in 
@breiman1996stacked.

Cross-validation is proven to be optimal for selection among estimators. This 
result was established through the oracle inequality for the cross-validation 
selector among a collection of candidate estimators [@vdl2003unified; 
@van2006oracle]. The only condition is that loss function is uniformly bounded, 
which is guaranteed in `sl3`.

<!--
The *oracle results* prove that, if the number of algorithms in the library are
polynomial in sample size, then the cross-validation selector (i.e., discrete
Super Learner) (1) is equivalent with the oracle selector asymptotically (based
on sample of size of training samples), or (2) achieves the parametric rate (log
$n/n$) for convergence with respect to the loss-based dissimilarity (risk)
between a candidate estimate $\psi$ and the true parameter value $\psi_0$.
-->

#### General Overview of the Algorithm

**What is cross-validation and how does it work?**

There are many different cross-validation schemes, designed to accommodate
different study designs and data structures. The figure below shows an example 
of 10-fold cross-validation.   
  
```{r cv_fig, echo = FALSE}
knitr::include_graphics("img/misc/vs.pdf")
```

**General step-by-step overview of the Super Learner algorithm:**

- Break up the sample evenly into V-folds (say V=10).
- For each of these 10 folds, remove that portion of the sample (kept out as
  validation sample) and the remaining will be used to fit learners (training
  sample).
- Fit each learner on the training sample (note, some learners will have their
  own internal cross-validation procedure or other methods to select tuning
  parameters).
- For each observation in the corresponding validation sample, predict the outcome
  using each of the learners, so if there are $p$ learners, then there would be
  $p$ predictions.
- Take out another validation sample and repeat until each of the V-sets of data
  are removed.
- Compare the cross-validated fit of the learners across all observations based
  on specified loss function (e.g., squared error, negative log-likelihood, ...)
  by calculating the corresponding average loss (risk).
- Either:

  + choose the learner with smallest risk and apply that learner to entire data
    set (resulting SL fit),
  + do a weighted average of the learners to minimize the cross-validated risk
    (construct an ensemble of learners), by

    + re-fitting the learners on the original data set, and
    + use the weights above to get the SL fit.

This entire procedure can be itself cross-validated to get a consistent 
estimate of the future performance of the Super Learner, and we implement this 
procedure later in this chapter. 

```{r cv_fig2, echo = FALSE}
knitr::include_graphics("img/misc/SLKaiserNew.pdf")
``` 

### Super Learner for Prediction

Say we observe a learning data set $X_i=(Y_i,W_i)$, for $i=1, ..., n$, where 
$Y_i$ is the outcome of interest, $W_i$ is a $p$-dimensional set of 
covariates, and our objective is to estimate the function $\psi_0(W) = E(Y|W)$.
This function can be expressed as the minimizer of the expected loss:
$\psi_0(W) = \text{argmin}_{\psi} E[L(X,\psi(W))]$. Here, the loss function is 
represented as $L$ (e.g., squared error loss, $L: (Y-\psi(W))^2)$).
  
For prediction, one can use the cross-validated risk to empirically determine
the relative performance of the Super Learner . When we 
have tested different algorithms on actual data and looked at the performance 
(e.g., MSE of prediction), never does one algorithm always win. Below, we show 
the results of such a study, comparing the fits of several different learners, 
including the SL algorithms.

```{r cv_fig3, echo = FALSE}
knitr::include_graphics("img/misc/ericSL.pdf")
```

For more detail on Super Learner we refer the reader to @vdl2007super and
@polley2010super. The optimality results for the cross-validation selector
among a family of algorithms were established in @vdl2003unified and extended
in @van2006oracle.
  
## `sl3` "Microwave Dinner" Implementation

We begin by illustrating the core functionality of the super learner algorithm
as implemented in `sl3`. For those who are interested in the internals 
of `sl3`, see this [`sl3` introductory 
tutorial](https://tlverse.org/sl3/articles/intro_sl3.html). 

The `sl3` implementation consists of the following steps:

0. Load the necessary libraries and data
1. Define the machine learning task
2. Make a super learner by creating library of base learners and a metalearner
3. Train the super learner on the machine learning task
4. Obtain predicted values

### WASH Benefits Study Example

Using the WASH data, we are interested in predicting weight-for-height z-score
`whz` using the available covariate data. More information on this dataset, and 
all other data that we will work with in this handbook, is contained in 
[Chapter 3]{#data}. Let's begin!

### 0. Load the necessary libraries and data {-}

First, we will load the relevant `R` packages, set a seed, and load the data.

If you would like to use newer `sl3` functionality that is available in the 
devel branch of the `sl3` GitHub repository, you need to install that version of 
the package (e.g. `devtools::install_github(tlverse/sl3@devel)`), re-start your 
`R` session, and then re-load the `sl3` package. 

```{r setup, message=FALSE, warning=FALSE}
library(data.table)
library(tidyverse)
library(origami)
library(SuperLearner)
library(sl3)
library(ggplot2)
library(knitr)
library(kableExtra)

set.seed(7194)
# my lucky seed! or is it 9174? or 4917? many lucky seeds, thanks lysdexia!

# load data set and take a peek
washb_data <- fread("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv",
  stringsAsFactors = TRUE
)
head(washb_data) %>%
  kable(digits = 4) %>%
  kableExtra:::kable_styling(fixed_thead = T) %>%
  scroll_box(width = "100%", height = "300px")
```

### 1. Define the machine learning task {-}

To define the machine learning **"task"** (predict weight-for-height z-score `whz`
using the available covariate data), we need to create an `sl3_Task` object. 

The `sl3_Task` keeps track of the roles the variables play in the
machine learning problem, the data, and any metadata (e.g., observational-level
weights, id, offset). 

Also, if we had missing outcomes, we would need to set 
`drop_missing_outcome = TRUE` when we create the task. In the next analysis, 
with the [IST stroke trial data](#ist), we do have a missing outcome. In the 
following chapter, we estimate the missingness mechanism and account for it in 
the TMLE. 

```{r task}
# specify the outcome and covariates
outcome <- "whz"
covars <- colnames(washb_data)[-which(names(washb_data) == outcome)]

# create the sl3 task
washb_task <- make_sl3_Task(
  data = washb_data,
  covariates = covars,
  outcome = outcome
)
```
*This warning is important.* The task just imputed missing covariates for us.
Specifically, for each covariate column with missing values, `sl3` uses the
median to impute missing continuous covariates, and the mode to impute binary 
and categorical covariates. 

Also, for each covariate column with missing values, `sl3` adds an additional 
column indicating whether or not the value was imputed, which is particularly 
handy when the missingness in the data might be informative.

Also, notice that we did not specify the number of folds, or the loss function
in the task. The default cross-validation scheme is V-fold, with the number of
folds $V=10$.

Let's visualize our `washb_task`.

```{r task-examine}
washb_task
```

### 2. Make a Super Learner {-}

Now that we have defined our machine learning problem with the task, we are
ready to **"make"** the Super Learner. This requires specification of

* A library of base learning algorithms that we think might be consistent with
  the true data-generating distribution.
* A metalearner, to ensemble the base learners.

We might also incorporate

* Feature selection, to pass only a subset of the predictors to the algorithm.
* Hyperparameter specification, to tune base learners.

Learners have properties that indicate what features they support. We may use
`sl3_list_properties()` to get a list of all properties supported by at least
one learner.

```{r list-properties}
sl3_list_properties()
```
Since we have a continuous outcome, we may identify the learners that support
this outcome type with `sl3_list_learners()`.

```{r list-learners}
sl3_list_learners("continuous")
```

Now that we have an idea of some learners, we can construct them using the
`make_learner` function.

```{r baselearners}
# choose base learners
lrnr_glm <- make_learner(Lrnr_glm)
lrnr_mean <- make_learner(Lrnr_mean)
```
We can customize learner hyperparameters to incorporate a diversity of different
settings. Documentation for the learners and their hyperparameters can be found
in the [`sl3` Learners
Reference](https://tlverse.org/sl3/reference/index.html#section-sl-learners).

```{r extra-lrnr-awesome, message=FALSE, warning=FALSE}
lrnr_ranger50 <- make_learner(Lrnr_ranger, num.trees = 50)
lrnr_hal_simple <- make_learner(Lrnr_hal9001, max_degree = 2, n_folds = 2)
lrnr_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1
lrnr_ridge <- make_learner(Lrnr_glmnet, alpha = 0)
lrnr_elasticnet <- make_learner(Lrnr_glmnet, alpha = .5)
```

We can also include learners from the `SuperLearner` `R` package.

```{r extra-lrnr-woah, message=FALSE, warning=FALSE}
lrnr_bayesglm <- Lrnr_pkg_SuperLearner$new("SL.bayesglm")
```

Here is a fun trick to create customized learners over a grid of parameters.

```{r extra-lrnr-mindblown-svm, eval = FALSE}
# I like to crock pot my super learners
grid_params <- list(
  cost = c(0.01, 0.1, 1, 10, 100, 1000),
  gamma = c(0.001, 0.01, 0.1, 1),
  kernel = c("polynomial", "radial", "sigmoid"),
  degree = c(1, 2, 3)
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
params_default <- list(nthread = getOption("sl.cores.learners", 1))
svm_learners <- apply(grid, MARGIN = 1, function(params_tune) {
  do.call(Lrnr_svm$new, c(params_default, as.list(params_tune)))
})
```
```{r extra-lrnr-mindblown-xgboost}
grid_params <- list(
  max_depth = c(2, 4, 6, 8),
  eta = c(0.001, 0.01, 0.1, 0.2, 0.3),
  nrounds = c(20, 50)
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
params_default <- list(nthread = getOption("sl.cores.learners", 1))
xgb_learners <- apply(grid, MARGIN = 1, function(params_tune) {
  do.call(Lrnr_xgboost$new, c(params_default, as.list(params_tune)))
})
```

Did you see `Lrnr_caret` when we called `sl3_list_learners(c("binomial"))`? 
All we need to specify is the algorithm to use, which is passed as `method` to 
`caret::train()`. The default method for parameter selection criterion with 
is set to "CV" instead of the `caret::train()` default `boot`. The summary 
metric to used to select the optimal model is `RMSE` for continuous outcomes 
and `Accuracy` for categorical and binomial outcomes.

```{r carotene, eval = FALSE}
# I have no idea how to tune a neural net (or BART machine..)
lrnr_caret_nnet <- make_learner(Lrnr_caret, algorithm = "nnet")
lrnr_caret_bartMachine <- make_learner(Lrnr_caret,
  algorithm = "bartMachine",
  method = "boot", metric = "Accuracy",
  tuneLength = 10
)
```

In order to assemble the library of learners, we need to **"stack"** them
together.

A `Stack` is a special learner and it has the same interface as all
other learners. What makes a stack special is that it combines multiple learners
by training them simultaneously, so that their predictions can be either
combined or compared.

```{r stack}
stack <- make_learner(
  Stack,
  lrnr_glm, lrnr_mean, lrnr_ridge, lrnr_lasso, xgb_learners[[10]]
)
```

We can optionally select a subset of available covariates and pass only
those variables to the modeling algorithm.

Let's consider screening covariates based on their `randomForest` variable 
importance ranking (ordered by mean decrease in accuracy). We select the top 5 
most important covariates according to this ranking, and we decreased the 
ntree to 20. 

Before you think it -- I will confess. Bob Ross and I both know that 20 trees 
makes for a lonely forest, and I shouldn't even consider it, but these are the 
sacrifices I have to make for this chapter to build in under 50 minutes! 

```{r screener}
screen_rf <- make_learner(Lrnr_screener_randomForest, nVar = 5, ntree = 20)
# which covariates are selected on the full data?
screen_rf$train(washb_task)
```

To **"pipe"** only the selected covariates to the modeling algorithm, we need to
make a `Pipeline`, which is a just set of learners to be fit sequentially, where
the fit from one learner is used to define the task for the next learner.

```{r screener-pipe}
screen_rf_pipeline <- make_learner(Pipeline, screen_rf, stack)
```
Now our learners will be preceded by a screening step.

We also consider the original `stack`, to compare how the feature selection
methods perform in comparison to the methods without feature selection, and 
because 

Analogous to what we have seen before, we have to stack the pipeline and
original `stack` together, so we may use them as base learners in our super
learner.

```{r screeners-stack, message=FALSE, warning=FALSE}
fancy_stack <- make_learner(Stack, screen_rf_pipeline, stack)
# we can visualize the stack
dt_stack <- delayed_learner_train(fancy_stack, washb_task)
plot(dt_stack, color = FALSE, height = "400px", width = "90%")
```

We will use the [default 
metalearner](https://github.com/tlverse/sl3/blob/master/R/default_metalearner.R), 
which uses [`Lrnr_solnp()`](https://github.com/tlverse/sl3/blob/master/R/Lrnr_solnp.R) 
to provide fitting procedures for a pairing of [loss 
function](https://github.com/tlverse/sl3/blob/master/R/loss_functions.R) and 
[metalearner 
function](https://github.com/tlverse/sl3/blob/master/R/metalearners.R). This
default metalearner selects a loss and metalearner pairing based on the outcome 
type. Note that any learner can be used as a metalearner.

We have made a library/stack of base learners, so we are ready to make the super 
learner. The super learner algorithm fits a metalearner on the validation-set 
predictions.

```{r make-sl, message=FALSE, warning=FALSE}
sl <- make_learner(Lrnr_sl,
  learners = fancy_stack
)
```
We can also use `Lrnr_cv` to build a super learner, cross-validate a stack of 
learners to compare performance of the learners in the stack, or cross-validate
any single learner (see "Cross-validation" section of this [`sl3` 
introductory tutorial](https://tlverse.org/sl3/articles/intro_sl3.html)).

Furthermore, we can [Define New `sl3` 
Learners](https://tlverse.org/sl3/articles/custom_lrnrs.html) which can be used 
in all the places you could otherwise use any other `sl3` learners, including
`Pipelines`, `Stacks`, and the Super Learner. 

In the plot below, we visualize the steps for executing the Super Learner in the 
`tlverse/delayed` framework. For those like myself who are not particularly 
keen on understanding the intricacies of `delayed`, let's focus on the main 
point of this figure: we can see there are 10 realizations of the stack which 
represent the 10 cross-validation folds and there is a separate hold-out 
(top branch of the figure) that will not be used to fit the Super Learner. 

```{r make-sl-plot, message=FALSE, warning=FALSE}
dt_sl <- delayed_learner_train(sl, washb_task)
plot(dt_sl, color = FALSE, height = "400px", width = "90%")
```

### 3. Train the Super Learner on the machine learning task {-}

The Super Learner algorithm fits a metalearner on the validation-set
predictions in a cross-validated manner, thereby avoiding overfitting. 

Now we are ready to **"train"** our Super Learner on our `sl3_task` object,
`washb_task`.

```{r sl}
sl_fit <- sl$train(washb_task)
```

### 4. Obtain predicted values {-}

Now that we have fit the super learner, we are ready to calculate the predicted
outcome for each subject.

```{r sl-predictions}
# we did it! now we have super learner predictions
sl_preds <- sl_fit$predict()
head(sl_preds)
```
<!--
Below we visualize the observed versus predicted values. 

For fun, we will also
include the cross-validated predictions from most popular learner on the block, 
main terms linear regression. 


```{r, plot-predvobs-woohoo}

df_plot <- data.frame(
  Observed = washb_data$whz,
  Predicted = sl_preds,
  count = c(1:nrow(washb_data))
)

df_plot_melted <- melt(df_plot,
  id.vars = "count",
  measure.vars = c("Observed", "Predicted")
)

ggplot(df_plot_melted, aes(value, count, color = variable)) +
  geom_point()
```
-->
We can also obtain a summary of the results.

```{r, sl-summary}
sl_fit_summary <- sl_fit$print()
```
From the table of the printed Super Learner fit, we note that the Super Learner
had a mean risk of `r sl_fit_summary$mean_risk[length(sl_fit_summary$learner)]`
and that this ensemble weighted the `ranger` and `glmnet` learners highest while
not weighting the `mean` learner highly. 

We can also see that the `glmnet` learner had the lowest cross-validated mean 
risk, thus making it the cross-validated selector (or the _discrete_ Super 
Learner). The mean risk of the Super Learner is calculated using the hold-out 
set that we visualized in the `dt_sl` plot. 

## Cross-validated Super Learner

We can cross-validate the Super Learner to see how well the Super Learner
performs on unseen data, and obtain an estimate of the cross-validated risk of
the Super Learner.

This estimation procedure requires an "external" layer of cross-validation,
also called nested cross-validation, which involves setting aside a separate
holdout sample that we don’t use to fit the Super Learner. This
external cross validation procedure may also incorporate 10 folds, which is the
default in `sl3`. However, we will incorporate 2 outer/external folds of
cross-validation for computational efficiency.

We also need to specify a loss function to evaluate Super Learner.
Documentation for the available loss functions can be found in the [`sl3` Loss
Function Reference](https://tlverse.org/sl3/reference/loss_functions.html).

```{r CVsl}
washb_task_new <- make_sl3_Task(
  data = washb_data,
  covariates = covars,
  outcome = outcome,
  folds = origami::make_folds(washb_data, fold_fun = folds_vfold, V = 2)
)
CVsl <- CV_lrnr_sl(sl_fit, washb_task_new, loss_squared_error)

# NOTE: kable doesn't work with PDF formatting
# CVsl %>%
#  kable(digits = 4) %>%
#  kableExtra:::kable_styling(fixed_thead = T) %>%
#  scroll_box(width = "100%", height = "300px")
```
<!-- TODO: Explain summary!!!! -->

## Variable Importance Measures with `sl3`

Variable importance can be interesting and informative. It can also be 
contradictory and confusing. Nevertheless, we like it, and so do 
collaborators, so we created a variable importance function in `sl3`! The `sl3` 
`varimp` function returns a table with variables listed in decreasing order of 
importance (i.e. most important on the first row). 

The measure of importance in `sl3` is based on a risk difference between the 
learner fit with a permuted covariate and the learner fit with the true 
covariate, across all covariates. In this manner, the larger the risk
difference, the more important the variable is in the prediction. 

The intuition of this measure is that it calculates the risk (in terms of the 
average loss in predictive accuracy) of losing one covariate, while keeping 
everything else fixed, and compares it to the risk if the covariate was not 
lost. If this risk difference is zero then losing that covariate had no 
impact, and is thus not important by this measure. We do this across all of the 
covariates. As stated above, we don't actually remove the covariate, we just 
permute/shuffle it, but the idea is that this shuffling distorts potentially
meaningful information that was present in the covariate. This idea of permuting
instead of removing saves a lot of time, and is also incorporated in the 
`randomForest` variable importance measures. 

Let's explore the `sl3` variable importance measurements for the `washb` data.

```{r varimp}
washb_varimp <- varimp(sl_fit, loss_squared_error)

# NOTE: kable doesn't work with PDF formatting
# washb_varimp %>%
#  kable(digits = 4) %>%
#  kableExtra:::kable_styling(fixed_thead = T) %>%
#  scroll_box(width = "100%", height = "300px")
```

```{r varimp-plot, warning = FALSE, message = FALSE}
# plot variable importance
washb_varimp %>%
  mutate(name = forcats::fct_reorder(X, risk_diff)) %>%
  ggplot(aes(x = risk_diff, y = name)) +
  geom_dotplot(binaxis = "y") +
  labs(
    x = "Risk Difference", y = "Covariate",
    title = "sl3 Variable Importance for WASH Benefits Example Data"
  )
```
<!-- Explain summary!!!! -->

## Exercises

### Predicting Myocardial Infarction with `sl3` {#sl3ex1}

Follow the steps below to predict myocardial infarction (`mi`) using the
available covariate data. We thank Prof. David Benkeser at Emory University for
making the this Cardiovascular Health Study (CHS) data accessible.

```{r ex-setup, warning=FALSE, message=FALSE}
# load the data set
db_data <-
  url("https://raw.githubusercontent.com/benkeser/sllecture/master/chspred.csv")
chspred <- read_csv(file = db_data, col_names = TRUE)

# NOTE: kable doesn't work with PDF formatting
# take a quick peek
# head(chspred) %>%
#  kable(digits = 4) %>%
#  kableExtra:::kable_styling(fixed_thead = T) %>%
#  scroll_box(width = "100%", height = "300px")
```

1. Create an `sl3` task, setting myocardial infarction `mi` as the outcome and
   using all available covariate data.
2. Make a library of seven relatively fast base learning algorithms (i.e., do
   not consider BART or HAL). Customize hyperparameters for one of your
   learners. Feel free to use learners from `sl3` or `SuperLearner`. You may
   use the same base learning library that is presented above.
3. Incorporate feature selection with the screener `screen.corP`.
4. Fit the metalearning step with the default metalearner.
5. With the metalearner and base learners, make the Super Learner and train it
   on the task.
6. Print your Super Learner fit by calling `print()` with `$`.
7. Cross-validate your Super Learner fit to see how well it performs on unseen
   data. Specify `loss_squared_error` as the loss function to evaluate the
   Super Learner.
8. Use the `varimp()` function to identify the most important predictor of     
   myocardial infarction. 

### Predicting Recurrent Ischemic Stroke in an RCT with `sl3` {#sl3ex2}

For this exercise, we will work with a random sample of 5,000 patients who
participated in the International Stroke Trial (IST). This data is described in 
[Chapter 3.2 of the `tlverse` 
handbook](https://tlverse.org/tlverse-handbook/data.html#ist).

1. Train a Super Learner to predict recurrent stroke `DRSISC` with the available 
   covariate data (the 25 other variables). Of course, you can consider feature 
   selection in the machine learning algorithms. In this data, the outcome is 
   occasionally missing, so be sure to specify `drop_missing_outcome = TRUE` 
   when defining the task.
2. Use the SL-based predictions to calculate the area under the ROC curve (AUC).
3. Calculate the cross-validated AUC to evaluate the performance of the 
   Super Learner on unseen data. 
4. Which covariates are the most predictive of 14-day recurrent stroke, 
   according to `sl3` variable importance measures?

```{r ex-setup2, warning=FALSE, message=FALSE}
ist_data <- data.table(read.csv("https://raw.githubusercontent.com/tlverse/tlverse-handbook/master/data/ist_sample.csv"))

# number 3 help
ist_task_CVsl <- make_sl3_Task(
  data = ist_data,
  outcome = "DRSISC",
  covariates = colnames(ist_data)[-which(names(ist_data) == "DRSISC")],
  drop_missing_outcome = TRUE,
  folds = origami::make_folds(
    n = sum(!is.na(ist_data$DRSISC)),
    fold_fun = folds_vfold,
    V = 5
  )
)
```

<!--
## Super Learning of a Conditional Density

### Super learning of a conditional density

Suppose we want to construct a Super Learner of the conditional probability
$g_0(a\mid W)=P_0(A=a\mid W)$, where $a\in {\cal A}$. Let's denote the values of
$a$ with $\{0, 1, \ldots, K\}$. A valid loss function for the conditional
density is
$L(g)(O) = -\log g(A\mid W).$
That is, $g_0 = \arg\min_g P_0L(g)$, i.e., $g_0$ is the minimizer of the
expectation of the log-likelihood loss.

**Candidate estimators**

1. Candidate estimators based on multinomial logistic regression: To start
with, one can use existing parametric model based MLE and machine learning
algorithms in `R` that fit a multinomial regression. For example, parametric
model multinomial logistic regression is available in `R` so that one can
already build a rich library of such estimators based on  different candidate
parametric models. In addition, `polyclass()` is a multinomial logistic
regression machine learning algorithm in `R`.

2. Candidate estimators based on machine learning for multinomial logistic
regression: Secondly, one can use a machine learning algorithm such as
`polyclass()` in `R` that data adaptively fits a multinomial logistic
regression, which itself has tuning parameters, again generating a class of
candidate estimators.

3. Incorporating screening: Note that one can also marry any of these choices
with a screening algorithm, thereby creating more candidate estimators of
interest. The screening can be particularly important when there are many
variables.

4. Candidate estimators by fitting separate logistic regressions and using
post-normalization

* Code $A$ in terms of Bernoullis $B_k=I(A=k)$, $k=0,\ldots,K$.
* Construct an estimator $\bar{g}_{nk}$ of $\bar{g}_{0k}(W)\equiv P_0(B_k=1\mid
  W)$ using any of the logistic regression algorithms, for all $k=0,\ldots,K$.
* This implies an estimator
\[
g_n(a\mid W)=\frac{\bar{g}_{na}(W)}{\sum_{k=0}^K \bar{g}_{nk}(W)}.\]
* In other words, we simply normalize these separate logistic regression
estimators so that we obtain a valid conditional distribution.
* This generates an enormous amount of interesting algorithms, since we have
available the whole machine learning literature for binary outcome regression.

5. Candidate estimators by estimating the conditional "hazard" with pooled
logistic regression.
Note that
\[
g_0(a\mid W)=\lambda_0(a\mid W) S_0(a\mid W),\]
where \[
\lambda_0(a\mid W)=P_0(A=a\mid A\geq a,W),\]

and $S_0(a\mid W)=\prod_{s\leq a}(1-\lambda_0(s\mid W))$ is the conditional
survival function $P_0(A>a\mid W)$. So we have now parameterized the
conditional distribution of $A$, given $W$, by a conditional hazard
$\lambda_0(a\mid W)$: $g_0=g_{\lambda_0}$.

* We could now focus on constructing candidate estimators of
$\lambda_0(a\mid W)$, which implies candidate estimators of $g_0$.

* For every observation $A_i$, we can create $A_i+1$ rows of data
$(W,s,I(A_i=s))$, $s=0,\ldots,A_i$, $i=1,\ldots,n$. We now run a logistic
regression estimator based on the pooled data set, ignoring ID, where we
regress the binary outcome $I(A_i=s)$ on the covariates $(W,s)$.

* If one assumes a parametric model, then this is nothing else then using the
maximum likelihood estimator, demonstrating that ignoring the ID is not
inefficient.

* This defines now an estimator of $\lambda_0(s\mid W)=P_0(A=s\mid W,A\geq s)$
as a function of $(s,W)$.  

* Different choices of logistic regression based estimators will define
different estimators.

* The pooling across $s$ is not very sensible if $A$ is not an ordered variable
If $A$ is categorical, we recommend to compute  a separate logistic regression
estimator of $\lambda_0(a\mid W)$ for each $a$ (i.e., stratify by $s$ in the
  above pooled data set).

* For non-categorical $A$, one could include both stratified (by level) as well
as pooled (across levels) based logistic regression estimators.


## Exercise 2 -- Estimating the Propensity Score with `sl3` {#sl3ex3}

exercise where we can look at positivity and maybe modify target population,
address issues related to this

## Super Learning of an Optimal Individualized Treatment Rule

* Data $O=(W,A,Y)$, and nonparametric model \mathcal{M} potentially containing
assumptions on the conditional probability distribution of $A$ given $W$
$g_0(A\mid W)$.
* Target: Optimal treatment rule $\psi_0(W)=I(B_0(W)>0)$, where
$B_0(W)=E_0(Y\mid A=1,W)-E_0(Y\mid A=0,W)$, the conditional treatment effect.
* Possible loss function for $\psi_0$ is an IPCW-loss:
\[
L_{g_0}(\psi)=\frac{I(A=\psi(W))}{g(A\mid W)}Y.\]

Indeed, $\psi_0$ is the minimizer of $EL_{g_0}(\psi)$ over all rules $\psi$.
* Construct library of candidate estimators of $\psi_0=I(B_0>0)$. This can
include estimators based on plugging in an estimator of $B_0$.
* One could also include a candidate estimator $I(B_n>0)$ where $B_n$ is a
Super Learner of $B_0$, e.g. based on loss function
\[
L_{g_0}(B)=\big(\frac{2A-1}{/g(A\mid W)}Y-B(W)\big)^2\]
that directly targets $B_0=\arg\min_B P_0L_{g_0}(B)$. This loss function is
still a squared error loss but its minimized by the true $B_0$.
* Estimate $g_0$ if not known.
* Compute cross-validation selector:
\[
k_n=\arg\min_k E_{B_n}P_{n,B_n}^1 L_{\hat{g}(P_{n,B_n}^0)}
(\hat{\Psi}_k(P_{n,B_n}^0)).\]
where $B_n = \{0,1\}^n$ is used for a binary vector of $n$ defining sample
splits, where the validation sample is ${i:B_n(i) = 1}$ and ${i:B_n(i) = 0}$ is
the training sample. The empirical distribution $P_{n,B_n}^0$ corresponds to
the split $B_n$ of the training sample and the empirical distribution of the
validation sample is $P_{n,B_n}^1$.
* Super-learner of optimal rule $\psi_0$: $\hat{\Psi}_{k_n}(P_n)$.

## Exercise 3 -- Estimating the Blip {#sl3ex4}

-->

## Concluding Remarks

* The general ensemble learning approach of Super Learner can be applied to a
diversity of estimation and prediction problems that can be defined by a loss
function. 

* We just discussed conditional mean estimation, outcome prediction and 
variable importance. In future updates of this chapter, we will delve into 
prediction of a conditional density, and the optimal individualized treatment 
rule. 

* If we plug in the estimator returned by super learner into the target
  parameter mapping, then we would end up with an estimator that has the same
  bias as what we plugged in, and would not be asymptotically linear. It also 
  would not be a plug-in estimator or efficient. 
  
  + An asymptotically linear estimator is important to have, since 
  they converge to the estimand at $\frac{1}{\sqrt{n}}$ rate, and thereby permit 
  formal statistical inference (i.e. confidence intervals and $p$-values).
  + Plug-in estimators of the estimand are desirable because they respect both 
  the local and global constraints of the statistical model (e.g. bounds), and
  have they have better finite-sample properties. 
  + An efficient estimator is optimal in the sense that it has the lowest 
  possible variance, and is thus the most precise. An estimator is efficient if 
  and only if is asymptotically linear with influence curve equal to the 
  canonical gradient. The canonical gradient is a mathematical object that is 
  specific to the target estimand, and it provides information on the level of 
  difficulty of the estimation problem. The canonical gradient is shown in the 
  chapters that follow. Practitioner's do not need to know how to calculate a 
  canonical gradient in order to understand efficiency and use Targeted Maximum 
  Likelihood Estimation (TMLE). Metaphorically, you do not need to be Yoda in 
  order to be a Jedi.
  
* TMLE is a general strategy that succeeds in constructing efficient and 
  asymptotically linear plug-in estimators. 

* Super Learner is fantastic for pure prediction, and for obtaining an initial 
  estimate in the first step of TMLE, but we need the second step of TMLE to
  have the desirable statistical properties mentioned above. 

* In the chapters that follow, we focus on the targeted maximum likelihood
  estimator and the targeted minimum loss-based estimator, both referred to as
  TMLE.

<!--
We could just plug-in the estimator returned by Super
Learner; however, this is problematic because the Super Learner estimators are
trading off bias and variance in an optimal way and as a result their bias is
essentially the rate of convergence of these algorithms, which is always slower
than $1/\sqrt{n}$. Therefore, if we plug-in the estimator returned by super
learner into the target parameter mapping, we would end up with an
estimator which has the same bias as what we plugged in, which is greater than
$1/\sqrt{n}$. Thus, we end up with an estimator which is not asymptotically
normal, since it does not converge to the estimand at $1/\sqrt{n}$ rate.

An asymptotically linear estimator has no meaningful bias ($ < 1/\sqrt{n}$), and
can be written as an empirical mean in first order of a function of the data,
the influence curve, plus some negligible remainder term. Once an estimator
is asymptotically linear with an influence curve it’s normally distributed, so
the standardized estimator converges to a normal distribution with mean 0 and
variance is the variance of the influence curve. Thus, it is advantageous to
construct asymptotically linear estimators since they permit formal statistical
inference. Among the class of regular asymptotically linear estimators, there is
an optimal estimator which is an efficient estimator, and that’s the one with
influence curve equal to the canonical gradient of the path-wise derivative of
the target parameter. The canonical gradient is the direction of the path
through the data distribution where the parameter is steepest. An estimator is
efficient if and only if is asymptotically linear with influence curve equal to
the canonical gradient. One can calculate the canonical gradient with the
statistical model and the statistical target parameter. Techniques for
calculating the canonical gradient entail projecting an initial gradient on the
tangent space of the model at the particular distribution in the model in which
you want to calculate the canonical gradient.

Now we know what it takes to construct an efficient estimator. Namely, we need
to construct an estimator which is asymptotically linear with influence curve
the canonical gradient. There are three general classes of estimators which
succeed in constructing asymptotically linear estimators: (1) the one-step
estimator, but it is not a plug-in estimator; (2) the targeted maximum
likelihood estimator, which is a Super Learner targeted towards the target
parameter and it is a plug-in estimator; and (3) estimating equation based
estimators, which use the canonical gradient but as an estimating function in
the target parameter. In the chapters that follow, we focus on the targeted
maximum likelihood estimator and the targeted minimum loss-based estimator,
both referred to as TMLE.
-->
## Appendix

### Exercise 1 Solution

Here is a potential solution to the [`sl3` Exercise 1 -- Predicting Myocardial
Infarction with `sl3`](#sl3ex1).

```{r ex-key, eval=FALSE, message=FALSE, warning=FALSE}
library(sl3)
library(origami)
library(data.table)
library(tidyverse)
library(ggplot2)

db_data <-
  url("https://raw.githubusercontent.com/benkeser/sllecture/master/chspred.csv")
chspred <- read_csv(file = db_data, col_names = TRUE)

# make task
chspred_task <- make_sl3_Task(
  data = chspred,
  covariates = head(colnames(chspred), -1),
  outcome = "mi"
)

# make learners
glm_learner <- Lrnr_glm$new()
lasso_learner <- Lrnr_glmnet$new(alpha = 1)
ridge_learner <- Lrnr_glmnet$new(alpha = 0)
enet_learner <- Lrnr_glmnet$new(alpha = 0.5)
# curated_glm_learner uses formula = "mi ~ smoke + beta + waist"
curated_glm_learner <- Lrnr_glm_fast$new(covariates = c("smoke, beta, waist"))
mean_learner <- Lrnr_mean$new() # That is one mean learner!
glm_fast_learner <- Lrnr_glm_fast$new()
ranger_learner <- Lrnr_ranger$new()
svm_learner <- Lrnr_svm$new()
xgb_learner <- Lrnr_xgboost$new()

# screening
screen_cor <- make_learner(Lrnr_screener_corP)
glm_pipeline <- make_learner(Pipeline, screen_cor, glm_learner)

# stack learners together
stack <- make_learner(
  Stack,
  glm_pipeline, glm_learner,
  lasso_learner, ridge_learner, enet_learner,
  curated_glm_learner, mean_learner, glm_fast_learner,
  ranger_learner, svm_learner, xgb_learner
)

# make and train super learner
sl <- Lrnr_sl$new(
  learners = stack
)
sl_fit <- sl$train(chspred_task)
sl_fit$print()

CVsl <- CV_lrnr_sl(sl_fit, chspred_task, loss_squared_error)
CVsl

importance <- varimp(sl_fit, loss_squared_error)
importance %>%
  mutate(name = forcats::fct_reorder(X, risk_diff)) %>%
  ggplot(aes(x = risk_diff, y = name)) +
  geom_dotplot(binaxis = "y") +
  labs(
    x = "Risk Difference", y = "Covariate",
    title = "sl3 Variable Importance for Myocardian Infarction Prediction"
  )
```

### Exercise 2 Solution

Here is a potential solution to the [`sl3` Exercise 2 -- Predicting Recurrent
Ischemic Stroke in an RCT with `sl3`](#sl3ex2).

```{r ex2-key, eval=FALSE}
library(sl3)
library(origami)
library(data.table)
library(tidyverse)
library(ROCR) # for AUC calculation
library(ggplot2)

ist_data <- data.table(read.csv("https://raw.githubusercontent.com/tlverse/tlverse-handbook/master/data/ist_sample.csv"))

# stack
ist_task <- make_sl3_Task(
  data = ist_data,
  outcome = "DRSISC",
  covariates = colnames(ist_data)[-which(names(ist_data) == "DRSISC")],
  drop_missing_outcome = TRUE
)

# learner library
lrnr_glm <- Lrnr_glm$new()
lrnr_lasso <- Lrnr_glmnet$new(alpha = 1)
lrnr_ridge <- Lrnr_glmnet$new(alpha = 0)
lrnr_enet <- Lrnr_glmnet$new(alpha = 0.5)
lrnr_mean <- Lrnr_mean$new()
lrnr_ranger <- Lrnr_ranger$new()
lrnr_svm <- Lrnr_svm$new()
# xgboost grid
grid_params <- list(
  max_depth = c(2, 5, 8),
  eta = c(0.01, 0.15, 0.3)
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
params_default <- list(nthread = getOption("sl.cores.learners", 1))
xgb_learners <- apply(grid, MARGIN = 1, function(params_tune) {
  do.call(Lrnr_xgboost$new, c(params_default, as.list(params_tune)))
})
learners <- unlist(list(
  xgb_learners, lrnr_ridge, lrnr_mean, lrnr_lasso,
  lrnr_glm, lrnr_enet, lrnr_ranger, lrnr_svm
),
recursive = TRUE
)

# super learner
sl <- Lrnr_sl$new(learners)
sl_fit <- sl$train(ist_task)

# AUC
preds <- sl_fit$predict()
obs <- c(na.omit(ist_data$DRSISC))
AUC <- performance(prediction(sl_preds, obs), measure = "auc")@y.values[[1]]
plot(performance(prediction(sl_preds, obs), "tpr", "fpr"))

# CVsl
ist_task_CVsl <- make_sl3_Task(
  data = ist_data,
  outcome = "DRSISC",
  covariates = colnames(ist_data)[-which(names(ist_data) == "DRSISC")],
  drop_missing_outcome = TRUE,
  folds = origami::make_folds(
    n = sum(!is.na(ist_data$DRSISC)),
    fold_fun = folds_vfold,
    V = 5
  )
)
CVsl <- CV_lrnr_sl(sl_fit, ist_task_CVsl, loss_loglik_binomial)
CVsl

# sl3 variable importance plot
importance <- varimp(sl_fit, loss_loglik_binomial)

importance %>%
  mutate(name = forcats::fct_reorder(X, risk_diff)) %>%
  ggplot(aes(x = risk_diff, y = name)) +
  geom_dotplot(binaxis = "y") +
  labs(
    x = "Risk Difference", y = "Covariate",
    title = "sl3 Variable Importance for Predicting Recurrent Ischemic Stroke"
  )
```

<!--
### Exercise 3 Solution

Here's a potential solution to the (Exercise 3)[@sl3ex3].

```{r ex3-key, eval=FALSE}

```
-->

<!--chapter:end:05-sl3.Rmd-->

# The TMLE Framework {#tmle3}

_Jeremy Coyle_

Based on the [`tmle3` `R` package](https://github.com/tlverse/tmle3).

## Learning Objectives
1. Use `tmle3` to estimate an Average Treatment Effect (ATE)
2. Understand `tmle3` "Specs"
3. Fit `tmle3` for a custom set of parameters
4. Use the delta method to estimate transformations of parameters

## Introduction

The first step in the estimation procedure is an initial estimate of the
data-generating distribution, or the relevant part of this distribution that is
needed to evaluate the target parameter. For this initial estimation, we use the
super learner [@vdl2007super], as described in the previous section.

With the initial estimate of relevant parts of the data-generating distribution
necessary to evaluate the target parameter, we are ready to construct the TMLE!

### Substitution Estimators

- Beyond a fit of the prediction function, one might also want to estimate more
  targeted parameters specific to certain scientific questions.
- The approach is to plug into the estimand of interest estimates of the
  relevant distributions.
- Sometimes, we can use simple empirical distributions, but averaging some
  function over the observations (e.g., giving weight $1/n$ for all
  observations).
- Other parts of the distribution, like conditional means or probabilities, the
  estimate will require some sort of smoothing due to the curse of
  dimensionality.

We give one example using an example of the average treatment effect (see
above):

- $\Psi(P_0) = \Psi(Q_0) = \mathbb{E}_0 \big[\mathbb{E}_0[Y \mid A = 1, W] -
  \mathbb{E}_0[Y \mid A = 0, W]\big]$, where $Q_0$ represents both the
  distribution of $Y \mid A,W$ and distribution of $W$.
- Let $\bar{Q}_0(A,W) \equiv \mathbb{E}_0(Y \mid A,W)$ and $Q_{0,W}(w) = P_0
  (W=w)$, then
  \[
  \Psi(Q_0) = \sum_w \{ \bar{Q}_0(1,w)-\bar{Q}_0(0,w)\} Q_{0,W}(w)
  \]
- The **Substitution Estimator** plugs in the empirical distribution (weight
  $1/n$ for each observation) for $Q_{0,W}(W_i)$, and some estimate of the
  regression of $Y$ on $(A,W)$  (say SL fit):
   \[
   \Psi(Q_n) = \frac{1}{n} \sum_{i=1}^n  \{ \bar{Q}_n(1,W_i)-\bar{Q}_n(0,W_i)\}
   \]
 - Thus, it becomes the average of the differences in predictions from the fit
   keeping the observed $W$, but first replacing $A=1$ and then the same but
   all $A=0$.

### TMLE

- Though using SL over an arbitrary parametric regression is an improvement,
  it's not sufficient to have the properties of an estimator one needs for
  rigorous inference.
- Because the variance-bias trade-off in the SL is focused on the prediction
  model, it can, for instance, under-fit portions of the distributions that are
  critical for estimating the parameter of interest, $\Psi(P_0)$.
- TMLE keeps the benefits of substitution estimators (it is one), but augments
  the original estimates to correct for this issue and also results in an
  asymptotically linear (and thus normally-distributed) estimator with
  consistent Wald-style confidence intervals.
- Produces a well-defined, unbiased, efficient substitution estimator of target
  parameters of a data-generating distribution.
- Updates an initial (super learner) estimate  of the relevant part of the
  data-generating distribution possibly using an estimate of a nuisance
  parameter (like the model of intervention given covariates).
- Removes asymptotic residual bias of initial estimator for the target
  parameter, if it uses a consistent estimator of $g_0$.
- If initial estimator was consistent for the target parameter, the additional
  fitting of the data in the targeting step may remove finite sample bias, and
  preserves consistency property of the initial estimator.
- If the initial estimator and the estimator of $g_0$ are both consistent, then
  it is also asymptotically  efficient according to semi-parametric statistical
  model efficiency theory.
- Thus, every effort is made to achieve minimal bias and the asymptotic
  semi-parametric efficiency bound for the variance.

```{r cv_fig4, echo = FALSE}
knitr::include_graphics("img/misc/TMLEimage.pdf")
```

- There are different types of TMLE, sometimes for the same set of parameters,
  but below is an example of the algorithm for estimating the ATE.
- In this case, one can present the estimator as:

\[
 \Psi(Q^{\star}_n) = \frac{1}{n} \sum_{i=1}^n \{ \bar{Q}^{\star}_n(1,W_i) -
 \bar{Q}^{\star}_n(0,W_i)\}
 \]
where $\bar{Q}^{\star}_n(A,W)$ is the TMLE augmented estimate.
$f(\bar{Q}^{\star}_n(A,W)) = f(\bar{Q}_n(A,W)) + \epsilon_n \cdot h_n(A,W)$,
where $f(\cdot)$ is the appropriate link function (e.g., logit), $\epsilon_n$
is an estimated coefficient and $h_n(A,W)$ is a "clever covariate".

- In this case, $h_n(A,W) = \frac{A}{g_n(W)}-\frac{1-A}{1-g_n(W)}$, with $g_n(W)
  = \mathbb{P}(A=1 \mid W)$ being the estimated (also by SL) propensity score,
  so the estimator depends both on initial SL fit of the outcome regression
  ($\bar{Q}_0$) and an SL fit of the propensity score ($g_n$).
- There are further robust augmentations that are used in `tlverse`, such as an
  added layer of cross-validation to avoid over-fitting bias (CV-TMLE), and so
  called methods that can more robustly estimated several parameters
  simultaneously (e.g., the points on a survival curve).

### Inference

- The estimators we discuss are **asymptotically linear**, meaning that the
  difference in the estimate $\Psi(P_n)$ and the true parameter ($\Psi(P_0)$)
  can be represented in first order by a i.i.d. sum:
  \begin{equation}\label{eqn:IC}
    \Psi(P_n) - \Psi(P_0) = \frac{1}{n} \sum_{i=1}^n IC(O_i; \nu) +
    o_p(1/\sqrt{n})
  \end{equation}
  where $IC(O_i; \nu)$ (the influence curve or function) is a function of the
  data and possibly other nuisance parameters $\nu$. Importantly, such
  estimators have mean-zero Gaussian limiting distributions; thus, in the
  univariate case, one has that
  \begin{equation}\label{eqn:limit_dist}
    \sqrt{n}(\Psi(P_n) - \Psi(P_0)) \xrightarrow[]{D}N(0,\mathbb{V}
    IC(O_i; \nu)),
  \end{equation}
  so that inference for the estimator of interest may be obtained in terms of
  the influence function. For this simple case, a 95\% confidence interval may
  be derived as:
  \begin{equation}\label{eqn:CI}
    \Psi(P^{\star}_n) \pm z_{1 - \frac{\alpha}{2}}
    \sqrt{\frac{\hat{\sigma}^2}{n}},
  \end{equation}
  where $SE=\sqrt{\frac{\hat{\sigma}^2}{n}}$ and $\hat{\sigma}^2$ is the sample
  variance of the estimated IC's: $IC(O; \hat{\nu})$. One can use the functional
  delta method to derive the influence curve if a parameter of interest may be
  written as a function of other asymptotically linear estimators.
- Thus, we can derive robust inference for parameters that are estimated by
  fitting complex, machine learning algorithms and these methods are
  computationally quick (do not rely on re-sampling based methods like the
  bootstrap).

## Easy-Bake Example: `tmle3` for ATE

We'll illustrate the most basic use of TMLE using the WASH Benefits data
introduced earlier and estimating an Average Treatment Effect (ATE).

As a reminder, the ATE is identified with the following statistical parameter
(under assumptions): $ATE = \mathbb{E}_0(Y(1)-Y(0)) = \mathbb{E}_0
\left(\mathbb{E}_0[Y \mid A=1,W] - \mathbb{E}_0[Y \mid A=0,W] \right),$

### Load the Data

We'll use the same WASH Benefits data as the earlier chapters:

```{r tmle3-load-data}
library(here)
library(data.table)
library(tidyverse)
library(tmle3)
library(sl3)
washb_data <- fread("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv",
  stringsAsFactors = TRUE
)
```

### Define the variable roles

We'll use the common W (covariates), A (treatment/intervention), Y (outcome) data
structure. `tmle3` needs to know what variables in the dataset correspond to
each of these roles. We use a list of character vectors to tell it. We call this
a "Node List" as it corresponds to the nodes in a Directed Acyclic Graph (DAG),
a way of displaying causal relationships between variables.

```{r tmle3-node-list}
node_list <- list(
  W = c(
    "month", "aged", "sex", "momage", "momedu",
    "momheight", "hfiacat", "Nlt18", "Ncomp", "watmin",
    "elec", "floor", "walls", "roof", "asset_wardrobe",
    "asset_table", "asset_chair", "asset_khat",
    "asset_chouki", "asset_tv", "asset_refrig",
    "asset_bike", "asset_moto", "asset_sewmach",
    "asset_mobile"
  ),
  A = "tr",
  Y = "whz"
)
```

### Handle Missingness

Currently, missingness in `tmle3` is handled in a fairly simple way:

* Missing covariates are median (for continuous) or mode (for discrete)
  imputed, and additional covariates indicating imputation are generated
* Observations missing either treatment or outcome variables are excluded.

We implemented IPCW-TMLE to more efficiently handle missingness in the outcome
variable, and we plan to implement an IPCW-TMLE to handle missingness in the
treatment variable as well.

These steps are implemented in the `process_missing` function in `tmle3`:

```{r tmle3-process_missing}
processed <- process_missing(washb_data, node_list)
washb_data <- processed$data
node_list <- processed$node_list
```

### Create a "Spec" Object

`tmle3` is general, and allows most components of the TMLE procedure to be
specified in a modular way. However, most end-users will not be interested in
manually specifying all of these components. Therefore, `tmle3` implements a
`tmle3_Spec` object that bundles a set ofcomponents into a _specification_
that, with minimal additional detail, can be run by an end-user.

We'll start with using one of the specs, and then work our way down into the
internals of `tmle3`.

```{r tmle3-ate-spec}
ate_spec <- tmle_ATE(
  treatment_level = "Nutrition + WSH",
  control_level = "Control"
)
```

### Define the learners

Currently, the only other thing a user must define are the `sl3` learners used
to estimate the relevant factors of the likelihood: Q and g.

This takes the form of a list of `sl3` learners, one for each likelihood factor
to be estimated with `sl3`:

```{r tmle3-learner-list}
# choose base learners
lrnr_mean <- make_learner(Lrnr_mean)
lrnr_xgboost <- make_learner(Lrnr_xgboost)

# define metalearners appropriate to data types
ls_metalearner <- make_learner(Lrnr_nnls)
mn_metalearner <- make_learner(
  Lrnr_solnp, metalearner_linear_multinomial,
  loss_loglik_multinomial
)
sl_Y <- Lrnr_sl$new(
  learners = list(lrnr_mean, lrnr_xgboost),
  metalearner = ls_metalearner
)
sl_A <- Lrnr_sl$new(
  learners = list(lrnr_mean, lrnr_xgboost),
  metalearner = mn_metalearner
)
learner_list <- list(A = sl_A, Y = sl_Y)
```

Here, we use a Super Learner as defined in the previous chapter. In the future,
we plan to include reasonable defaults learners.

### Fit the TMLE

We now have everything we need to fit the tmle using `tmle3`:

```{r tmle3-spec-fit}
tmle_fit <- tmle3(ate_spec, washb_data, node_list, learner_list)
```

### Evaluate the Estimates

We can see the summary results by printing the fit object. Alternatively, we
can extra results from the summary by indexing into it:
```{r tmle3-spec-summary}
print(tmle_fit)

estimates <- tmle_fit$summary$psi_transformed
print(estimates)
```

## `tmle3` Components

Now that we've successfully used a spec to obtain a TML estimate, let's look
under the hood at the components. The spec has a number of functions that
generate the objects necessary to define and fit a TMLE.

### `tmle3_task`

First is, a `tmle3_Task`, analogous to an `sl3_Task`, containing the data we're
fitting the TMLE to, as well as an NP-SEM generated from the `node_list`
defined above, describing the variables and their relationships.

```{r tmle3-spec-task}
tmle_task <- ate_spec$make_tmle_task(washb_data, node_list)
```

```{r tmle3-spec-task-npsem}
tmle_task$npsem
```

### Initial Likelihood

Next, is an object representing the likelihood, factorized according to the
NPSEM described above:

```{r tmle3-spec-initial-likelihood}
initial_likelihood <- ate_spec$make_initial_likelihood(
  tmle_task,
  learner_list
)
print(initial_likelihood)
```

These components of the likelihood indicate how the factors were estimated: the
marginal distribution of $W$ was estimated using NP-MLE, and the conditional
distributions of $A$ and $Y$ were estimated using `sl3` fits (as defined with
the `learner_list`) above.

We can use this in tandem with the `tmle_task` object to obtain likelihood
estimates for each observation:
```{r tmle3-spec-initial-likelihood-estimates}
initial_likelihood$get_likelihoods(tmle_task)
```

<!-- TODO: make helper to get learners out of fit objects -->

### Targeted Likelihood (updater)

We also need to define a "Targeted Likelihood" object. This is a special type
of likelihood that is able to be updated using an `tmle3_Update` object. This
object defines the update strategy (e.g. submodel, loss function, CV-TMLE or
not, etc).

```{r tmle3-spec-targeted-likelihood}
targeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)
```

When constructing the targeted likelihood, you can specify different update
options. See the documentation for `tmle3_Update` for details of the different
options. For example, you can disable CV-TMLE (the default in `tmle3`) as
follows:

```{r tmle3-spec-targeted-likelihood-no-cv}
targeted_likelihood_no_cv <-
  Targeted_Likelihood$new(initial_likelihood,
    updater = list(cvtmle = FALSE)
  )
```

### Parameter Mapping

Finally, we need to define the parameters of interest. Here, the spec defines a
single parameter, the ATE. In the next section, we'll see how to add additional
parameters.

```{r tmle3-spec-params}
tmle_params <- ate_spec$make_params(tmle_task, targeted_likelihood)
print(tmle_params)
```

### Putting it all together

Having used the spec to manually generate all these components, we can now
manually fit a `tmle3`:

```{r tmle3-manual-fit}
tmle_fit_manual <- fit_tmle3(
  tmle_task, targeted_likelihood, tmle_params,
  targeted_likelihood$updater
)
print(tmle_fit_manual)
```

The result is equivalent to fitting using the `tmle3` function as above.

## Fitting `tmle3` with multiple parameters

Above, we fit a `tmle3` with just one parameter. `tmle3` also supports fitting
multiple parameters simultaneously. To illustrate this, we'll use the
`tmle_TSM_all` spec:

```{r tmle3-tsm-all}
tsm_spec <- tmle_TSM_all()
targeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)
all_tsm_params <- tsm_spec$make_params(tmle_task, targeted_likelihood)
print(all_tsm_params)
```

This spec generates a Treatment Specific Mean (TSM) for each level of the
exposure variable. Note that we must first generate a new targeted likelihood,
as the old one was targeted to the ATE. However, we can recycle the initial
likelihood we fit above, saving us a super learner step.

### Delta Method

We can also define parameters based on Delta Method Transformations of other
parameters. For instance, we can estimate a ATE using the delta method and two
of the above TSM parameters:

```{r tmle3-delta-method-param}
ate_param <- define_param(
  Param_delta, targeted_likelihood,
  delta_param_ATE,
  list(all_tsm_params[[1]], all_tsm_params[[4]])
)
print(ate_param)
```

This can similarly be used to estimate other derived parameters like Relative
Risks, and Population Attributable Risks

### Fit

We can now fit a TMLE simultaneously for all TSM parameters, as well as the
above defined ATE parameter

```{r tmle3-tsm-plus-delta}
all_params <- c(all_tsm_params, ate_param)

tmle_fit_multiparam <- fit_tmle3(
  tmle_task, targeted_likelihood, all_params,
  targeted_likelihood$updater
)

print(tmle_fit_multiparam)
```

## Exercises

### Estimation of the ATE with `tmle3` {#sl3ex2}

Follow the steps below to estimate an average treatment effect using data from
the Collaborative Perinatal Project (CPP), available in the `sl3` package. To
simplify this example, we define a binary intervention variable, `parity01` --
an indicator of having one or more children before the current child and a
binary outcome, `haz01` -- an indicator of having an above average height for
age.

```{r tmle-exercise-data, message=FALSE, warning=FALSE}
# load the data set
data(cpp)
cpp <- cpp[!is.na(cpp[, "haz"]), ]
cpp$parity01 <- as.numeric(cpp$parity > 0)
cpp[is.na(cpp)] <- 0
cpp$haz01 <- as.numeric(cpp$haz > 0)
```
<!--
We're interested in using this simplified data to estimate an Average Treatment
Effect (ATE):

$$\Psi(P_0) = E_0(E_0[Y|A=1,W]-E_0[Y|A=0,W])$$


The purely statistical (non-causal) parameter can be interpreted as the average
of the difference in means across the strata for $W$, and only requires the
positivity assumption, that the conditional treatment assignment probabilities
are positive for each possible $w: P_0(A=1 \mid W=w) > 0$ and
$P_0(A=0 \mid W=w) > 0$ for each possible $w$.

To interpret this parameter as causal, specifically the causal risk difference
$E_0Y_1-E_0Y_0$, then we would also need to make the randomization assumption
stating that $A$ is independent of the counterfactuals $(Y_0,Y_1)$ within
strata of $W$. This assumption might have been included in the original SCM
$\mathcal{M}^F$, but, if one knows there are unmeasured confounders, then the
model $\mathcal{M}^{F\*}$ would be more restrictive by enforcing this "known
to be wrong" randomization assumption. Still, this assumption does not change
the statistical model $\mathcal{M}$, and as a consequence, it does not affect
the estimation problem either. Thus, the theorem's that establish desirable
properties of the TMLE, still hold even when this non-testable randomization
assumption is violated.

We proceed with implementing a targeted minimum loss-based estimator (TMLE),
an efficient substitution estimator which is not only asymptotically
consistent, asymptotically normally distributed, and asymptotically efficient,
but also tailored to have robust finite sample performance.
-->

1. Define the variable roles $(W,A,Y)$ by creating a list of these nodes.
   Include the following baseline covariates in $W$: `apgar1`, `apgar5`,
   `gagebrth`, `mage`, `meducyrs`, `sexn`. Both $A$ and $Y$ are specified
   above.
2. Define a `tmle3_Spec` object for the ATE, `tmle_ATE()`.
3. Using the same base learning libraries defined above, specify `sl3` base
   learners for estimation of $Q = E(Y|A,Y)$ and $g=P(A|W)$.
4. Define the metalearner like below.

```{r, metalrnr-exercise}
metalearner <- make_learner(Lrnr_solnp,
  loss_function = loss_loglik_binomial,
  learner_function = metalearner_logistic_binomial
)
```

5. Define one super learner for estimating $Q$ and another for estimating $g$.
   Use the metalearner above for both $Q$ and $g$ super learners.
6. Create a list of the two super learners defined in Step 5 and call this
   object `learner_list`. The list names should be `A` (defining the super
   learner for estimating $g$) and `Y` (defining the super learner for
   estimating $Q$).
7. Fit the tmle with the `tmle3` function by specifying (1) the `tmle3_Spec`,
   which we defined in Step 2; (2) the data; (3) the list of nodes, which we
   specified in Step 1; and (4) the list of super learners for estimating $g$
   and $Q$, which we defined in Step 6. *Note*: Like before, you will need to
   make a data copy to deal with `data.table` weirdness
   (`cpp2 <- data.table::copy(cpp)`) and use `cpp2` as the data.

### Estimation of Strata-Specific ATEs with `tmle3` {#sl3ex2}

For this exercise, we will work with a random sample of 5,000 patients who
participated in the International Stroke Trial (IST). This data is described in
the [Chapter 3.2 of the `tlverse`
handbook](https://tlverse.org/tlverse-handbook/data.html#ist). We included the
data below and a summarized description that is relevant for this exercise.

The outcome, $Y$, indicates recurrent ischemic stroke within 14 days after
randomization (`DRSISC`); the treatment of interest, $A$, is the randomized
aspirin vs. no aspirin treatment allocation (`RXASP` in  `ist`); and the
adjustment set, $W$, consists simply of other variables measured at baseline. In
this data, the outcome is occasionally missing, but there is no need to create a
variable indicating this missingness (such as $\Delta$) for analyses in the
`tlverse`, since the missingness is automatically detected when `NA` are present
in the outcome. Covariates with missing values (`RATRIAL`, `RASP3` and `RHEP24`)
have already been imputed. Additional covariates were created
(`MISSING_RATRIAL_RASP3` and `MISSING_RHEP24`), which indicate whether or not
the covariate was imputed. The missingness was identical for `RATRIAL` and
`RASP3`, which is why only one covariate indicating imputation for these two
covariates was created.

1. Estimate the average effect of randomized asprin treatment (`RXASP` = 1) on
   recurrent ischemic stroke. Even though the missingness mechanism on $Y$,
   $\Delta$, does not need to be specified in the node list, it does still need
   to be accounted for in the TMLE. In other words, for this estimation problem,
   $\Delta$ is a relevant factor of the likelihood in addition to $Q$, $g$.
   Thus, when defining the list of `sl3` learners for each likelihood factor, be
   sure to include a list of learners for estimation of $\Delta$, say
   `sl_Delta`, and specify something like
   `learner_list <- list(A = sl_A, delta_Y = sl_Delta, Y = sl_Y)`.
2. Recall that this RCT was conducted internationally. Suposse there is concern
   that the dose of asprin may have varied across geographical regions, and an
   average across all geographical regions may not be warranted. Calculate the
   strata specific ATEs according to geographical region (`REGION`).

```{r tmle3-ex2}
ist_data <- data.table(read.csv("https://raw.githubusercontent.com/tlverse/deming2019-workshop/master/data/ist_sample.csv"))
```

## Summary

`tmle3` is a general purpose framework for generating TML estimates. The
easiest way to use it is to use a predefined spec, allowing you to just fill in
the blanks for the data, variable roles, and `sl3` learners. However, digging
under the hood allows users to specify a wide range of TMLEs. In the next
sections, we'll see how this framework can be used to estimate advanced
parameters such as optimal treatments and shift interventions.

<!--chapter:end:06-tmle3.Rmd-->

# Optimal Individualized Treatment Regimes

_Ivana Malenica_

Based on the [`tmle3mopttx` `R` package](https://github.com/tlverse/tmle3mopttx)
by _Ivana Malenica, Jeremy Coyle, and Mark van der Laan_.

Updated: `r Sys.Date()`

## Learning Objectives

1. Differentiate dynamic and optimal dynamic treatment regimes from static
   interventions.
2. Understand the benefits and challenges associated with using
   optimal individualized treatment regimes in practice.
3. Contrast the impact of implementing an optimal individualized treatment
   in the population with static and dynamic regimes.
4. Estimate causal effects under optimal individualized treatment regimes with
   the `tmle3mopttx` `R` package.
5. Contrast the population impact of implementing optimal individualized
   treatment based on sub-optimal rules.
6. Construct realistic optimal individualized treatments that respect real data
   and subject-matter knowledge limitations on interventions.
7. Understand and implement variable importance analysis defined in
   terms of optimal individualized treatment interventions.

## Introduction to Optimal Individualized Interventions

Identifying which intervention will be effective for which patient based on
lifestyle, genetic and environmental factors is a common goal in precision
medicine. To put it in context, Abacavir and Tenofovir are commonly prescribed
as part of the antiretroviral therapy to Human Immunodeficiency Virus (HIV)
patients. However, not all individuals benefit from the two medications equally.
In particular, patients with renal dysfunction might further deteriorate if
prescribed Tenofovir, due to the high nephrotoxicity caused by the medication.
While Tenofovir is still highly effective treatment option for HIV patients, in
order to maximize the patient's well-being, it would be beneficial to prescribe
Tenofovir only to individuals with healthy kidney function. Along the same
lines, one might seek to improve retention in HIV care. In a randomized clinical
trial, several interventions show efficacy- including appointment reminders
through text messages, small cash incentives for on time clinic visits, and peer
health workers. Ideally, we want to improve effectiveness by assigning each
patient the intervention they are most likely to benefit from, as well as
improve efficiency by not allocating resources to individuals that do not need
them, or would not benefit from it.

```{r, fig.cap="Illustration of a Dynamic Treatment Regime in a Clinical Setting", echo=FALSE, eval=TRUE, out.width='60%'}
knitr::include_graphics(path = "img/image/DynamicA_Illustration.png")
```

One opts to administer the intervention to individuals who will profit from it,
instead of assigning treatment on a population level. But how do we know which
intervention works for which patient? This aim motivates a different type of
intervention, as opposed to the static exposures we might be used to. In
particular, in this chapter we learn about dynamic or individualized interventions
that tailor the treatment decision based on the collected covariates. Formally,
dynamic treatments represent interventions that at each treatment-decision stage
are allowed to respond to the currently available treatment and covariate
history.

In the statistics community such a treatment strategy is termed an
__individualized treatment regime__ (ITR), and the (counterfactual) population
mean outcome under an ITR is the value of the ITR [@neyman1990; @robins1986;
@pearl2009]. Even more, suppose one wishes to maximize the population mean of an
outcome, where for each individual we have access to some set of measured
covariates. This means, for example, that we can learn for which individual
characteristics assigning treatment increases the probability of a beneficial
outcome for each individual. An ITR with the maximal value is referred to as an
optimal ITR or the __optimal individualized treatment__. Consequently, the value
of an optimal ITR is termed the optimal value, or the __mean under the optimal
individualized treatment__.

The problem of estimating the optimal individualized treatment has received much
attention in the statistics literature over the years, especially with the
advancement of precision medicine; see @murphy2003, @robins2004, @laber2012,
@kosorok2012, @moodie2013 and @robins2014 to name a few. However, much of the
early work depends on parametric assumptions. As such, even in a randomized
trial, the statistical inference for the optimal individualized treatment relies
on assumptions that are generally believed to be false, and can lead to biased
results.

In this chapter, we consider estimation of the mean outcome under the optimal
individualized treatment where the candidate rules are restricted to depend only
on user-supplied subset of the baseline covariates. The estimation problem is
addressed in a statistical model for the data distribution that is
nonparametric, and at most places restrictions on the probability of a patient
receiving treatment given covariates (as in a randomized trial). As such, we
don't need to make any assumptions about the relationship of the outcome with
the treatment and covariates, or the relationship between the treatment and
covariates. Further, we provide a Targeted Maximum Likelihood Estimator for the
mean under the optimal individualized treatment that allows us to generate valid
inference for our parameter, without having any parametric assumptions. For a
technical presentation of the algorithm, the interested reader is invited to
further consult @vanderLaanLuedtke15 and @luedtke2016super.

---

## Data Structure and Notation

Suppose we observe $n$ independent and identically distributed observations of
the form $O=(W,A,Y) \sim P_0$. We denote $A$ as categorical treatment, and $Y$
as the final outcome. In particular, we define $A \in \mathcal{A}$ where
$\mathcal{A} \equiv \{a_1, \cdots, a_{n_A} \}$ and $n_A = |\mathcal{A}|$, with
$n_A$ denoting the number of categories (possibly only two, for a binary setup).
Note that we treat $W$ as vector-valued, representing all of our collected
baseline covariates. Therefore, for a single random individual $i$, we have that
their observed data is $O_i$: with corresponding baseline covariates $W_i$,
treatment $A_i$, and final outcome $Y_i$. We say that $O \sim P_0$, or that all
data was drawn from some probability distribution $P_0$. We emphasize that we
make no assumptions about the distribution of $P_0$, so that $P_0 \in
\mathcal{M}$, where $\mathcal{M}$ is the fully nonparametric model. As
previously mentioned, this means that we make no assumptions on the relationship
between $Y$ and $A$ and $W$, but might be able to say something about the
relationship of $A$ and $W$, as is the case of a randomized trial. We can assume
a nonparametric structural equation model (NPSEM) to describe generation of $O$,
as described by @pearl2009causality. Specifically, we have that:
\begin{align*}\label{eqn:npsem}
  W &= f_W(U_W) \\ A &= f_A(W, U_A) \\ Y &= f_Y(A, W, U_Y),
\end{align*}
where the collection $f=(f_W,f_A,f_Y)$ denotes unspecified or partially
specified functions. In particular, NPSEM parameterizes $P_0$ in terms of the
distribution of random variables $O$ and $U$, where $U=(U_W,U_A,U_Y)$ are the
exogenous random variables. We can define counterfactuals $Y_{d(W)}$ defined by
a modified system in which the equation for $A$ is replaced by the rule $d(W)$,
dependent on covariates $W$.

The likelihood of the data admits a factorization, implied by the time ordering
of $O$. We denote the density of $O$ as $p_0$, corresponding to the distribution
$P_0$ and dominating measure $\mu$.
\begin{equation*}\label{eqn:likelihood_factorization}
  p_0(O) = p_{Y,0}(Y|A,W) p_{A,0}(A|W) p_{W,0}(W) = q_{Y,0}(Y|A,W) q_{A,0}(A|W)
    q_{W,0}(W),
\end{equation*}
where $p_{Y,0}(Y|A,W)$ is the conditional density of $Y$ given $(A, W)$ with
respect to some dominating measure $\mu_Y$, $p_{A,0}$ is the conditional density
of $A$ given $W$ with respect to dominating measure $\mu_A$, and $p_{W,0}$ is
the density of $W$ with respect to dominating measure $\mu_W$. Consequently, we
define $P_{Y,0}(Y|A,W)=Q_{Y,0}(Y|A,W)$, $P_{A,0}(A|W)=g_0(A|W)$ and
$P_{W,0}(W)=Q_{W,0}(W)$ as the corresponding conditional distributions of $Y$,
$A$ and $W$. For notational simplicity, we define $\bar{Q}_{Y,0}(A,W) \equiv
E_0[Y|A,W]$ as the conditional expectation of $Y$ given $(A,W)$.

In addition, we denote $V$ as $V \in W$, defining a subset of the baseline
covariates the optimal individualized rule depends on. Note that $V$ could be
all of $W$, or an empty set, depending on the subject matter knowledge. In
particular, a researcher might want to consider known effect modifiers available
at the time of treatment decision as possible $V$ covariates. Defining $V$
allows us to consider possibly sub-optimal rules that are easier to estimate,
and thereby allows for statistical inference for the counterfactual mean outcome
under the sub-optimal rule.

## Defining the Causal Effect of an Optimal Individualized Intervention

Consider dynamic treatment rules $V \rightarrow d(V) \in \{a_1, \cdots,
a_{n_A} \} \times \{1\}$, for assigning treatment $A$ based on $V \in W$. As
mentioned in the previous section, causal effects are defined in terms of
hypothetical interventions on the NPSEM (\ref{eqn:npsem}). Our modified system
then takes the following form:
\begin{align*}\label{eqn:npsem_causal}
  W &= f_W(U_W) \\ A &= d(V) \\ Y_{d(V)} &= f_Y(d(V), W, U_Y),
\end{align*}

where the dynamic treatment regime may be viewed as an intervention in which
$A$ is set equal to a value based on a hypothetical regime $d(V)$, and
$Y_{d(V)}$ is the corresponding outcome under $d(V)$. We denote the distribution
of the counterfactual quantities as $P_{0,d(V)}$.

The goal of any causal analysis motivated by such dynamic, or optimal
individualized intervention, is to estimate a parameter defined as the
counterfactual mean of the outcome with respect to the modified intervention
distribution (either dynamic or optimal dynamic). We are primarily interested in
the value of an individualized rule, $E_0[Y_{d(V)}]$. The optimal rule is the
rule with the maximal value: $$d_{opt}(V) \equiv \text{argmax}_{d(V) \in
\mathcal{D}} E_0[Y_{d(V)}]$$ where $\mathcal{D}$ represents the set of possible
rules, $d$, implied by $V$. We note that, in case the problem at hand requires
minimizing the mean of an outcome, our optimal individualized rule will be the
rule with the minimal value instead. Finally, our target parameter can be
expressed as
$$\psi_0 := E_0[Y_{d_{opt}(V)}].$$

The optimal individualized rule, as well as the value of a rule, are causal
parameters based on the unobserved counterfactuals. In order for the causal
quantities to be estimated from the observed data, they need to be identified
with statistical parameters. This step of the roadmap requires me make few
assumptions:

1. _Consistency_: $Y^{d(v_i)}_i = Y_i$ in the event $A_i = d(v_i)$,
   for $i = 1, \ldots, n$.
2. _Stable unit value treatment assumption (SUTVA)_: $Y^{d(v_i)}_i$ does
   not depend on $d(v_j)$ for $i = 1, \ldots, n$ and $j \neq i$, or lack
   of interference.
3. _Strong ignorability_: $A \perp \!\!\! \perp  Y^{d(v)} \mid W$, for all $a
   \in \mathcal{A}$.
4. _Positivity (or overlap)_: $P_0(\min_{a \in \mathcal{A}} g_0(a|W) > 0)=1$

Under the above causal assumptions, we can identify $P_{0,d}$ with observed data
using the G-computation formula:

$$P_{0,d_{opt}}(O) = Q_{Y,0}(Y|A=d_{opt}(V),W)g_0(A=d_{opt}(V)|W)Q_{W,0}(W).$$
The value of an individualized rule can now be expressed as

$$E_0[Y_{d(V)}] = E_{0,W}[\bar{Q}_{Y,0}(A=d(V),W)],$$

which, under causal assumptions, can is interpreted as the mean outcome if
(possibly contrary to fact), treatment was assigned according to the rule.
Finally, the statistical counterpart to the causal parameter of interest is
defined as
$$\psi_0 = E_{0,W}[\bar{Q}_{Y,0}(A=d_{opt}(V),W)].$$

Inference for the optimal value has been shown to be difficult at exceptional
laws, defined as probability distributions for which treatment is neither
beneficial nor harmful. Inference is similarly difficult in finite samples if
the treatment effect is very small in all strata, even though valid asymptotic
estimators exist in this setting. With that in mind, we address the estimation
problem under the assumption of non-exceptional laws in effect.

Many methods for learning the optimal rule from data have been developed
[@murphy2003, @robins2004, @laber2012, @kosorok2012, @moodie2013]. In this
chapter, we focus on the methods discussed in @luedtke2016super and
@vanderLaanLuedtke15. Note however, that `tmle3mopttx` also supports the widely
used Q-learning approach, where the optimal individualized rule is based on the
initial estimate of $\bar{Q}_{Y,0}(A,W)$ [@Sutton1998].

We follow the methodology outlined in @luedtke2016super and
@vanderLaanLuedtke15, where we learn the optimal ITR using Super Learner
[@vdl2007super], and estimate its value with cross-validated Targeted Minimum
Loss-based Estimation (CV-TMLE) [@cvtmle2010]. In great generality, we first
need to estimate the true individual treatment regime, $d_0(V)$, which
corresponds to dynamic treatment rule ($d(V)$) that takes a subset of covariates
$V \in W$ and assigns treatment to each individual based on their observed
covariates $v$. With the estimate of the true optimal ITR in hand, we can
estimate its corresponding value.

### Binary treatment

How do we estimate the optimal individualized treatment regime? In the case of a
binary treatment, a key quantity for optimal ITR is the blip function. One can
show that any optimal ITR assigns treatment to individuals falling in strata in
which the stratum specific average treatment effect, the blip function, is
positive and does not assign treatment to individuals for which this quantity is
negative. Therefore for a binary treatment, under causal assumptions, we define
the blip function as:
$$\bar{Q}_0(V) \equiv E_0[Y_1-Y_0|V] \equiv E_0[\bar{Q}_{Y,0}(1,W) -
\bar{Q}_{Y,0}(0,W) | V],$$
or the average treatment effect within a stratum of $V$. The note that the
optimal individualized rule can now be derived as $d_{opt}(V) =
I(\bar{Q}_{0}(V) > 0)$.

The package `tmle3mopttx` relies on using the Super Learner to estimate the blip
function, as it easily extends to more general categorical treatment. With that
in mind, the loss function utilized for learning the optimal individualized rule
corresponds to conditional mean type losses. It is however worth mentioning that
@luedtke2016super present three different approaches for learning the optimal
rule. Namely, they focus on:

1. Super Learning the Blip Function,

2. Super Learning the Weighted Classification Problem,

3. Joint Super Learner of the Blip and Weighted Classification Problem.

We refer the interested reader to @luedtke2016super for further reference on
advantages of each approach.

Relying on the Targeted Maximum Likelihood (TML) estimator and the Super Learner
estimate of the blip function, we follow the below steps in order to obtain
value of the ITR:

1. Estimate $\bar{Q}_{Y,0}(A,W)$ and $g_0(A|W)$ using `sl3`. We denote such
   estimates as $\bar{Q}_{Y,n}(A,W)$ and $g_n(A|W)$.
2. Apply the doubly robust Augmented-Inverse Probability Weighted (A-IPW)
   transform to our outcome, where we define:
   $$D_{\bar{Q}_Y,g,a}(O) \equiv \frac{I(A=a)}{g(A|W)} (Y-\bar{Q}_Y(A,W)) +
     \bar{Q}_Y(A=a,W)$$

Note that under the randomization and positivity assumptions we have that
$E[D_{\bar{Q}_Y,g,a}(O) | V] = E[Y_a |V]$. We emphasize the double robust nature
of the A-IPW transform- consistency of $E[Y_a |V]$ will depend on correct
estimation of either $\bar{Q}_{Y,0}(A,W)$ or $g_0(A|W)$. As such, in a
randomized trial, we are guaranteed a consistent estimate of $E[Y_a |V]$ even if
we get $\bar{Q}_{Y,0}(A,W)$ wrong!

Using this transform, we can define the following contrast:
$D_{\bar{Q}_Y,g}(O) = D_{\bar{Q}_Y,g,a=1}(O) - D_{\bar{Q}_Y,g,a=0}(O)$

We estimate the blip function, $\bar{Q}_{0,a}(V)$, by regressing
$D_{\bar{Q}_Y,g}(O)$ on $V$ using the specified `sl3` library of learners and an
appropriate loss function.

3. Our estimated rule is $d(V) = \text{argmax}_{a \in \mathcal{A}}
   \bar{Q}_{0,a}(V)$.
4. We obtain inference for the mean outcome under the estimated optimal rule
   using CV-TMLE.

### Categorical treatment

In line with the approach considered for binary treatment, we extend the blip
function to allow for categorical treatment. We denote such blip function
extensions as _pseudo-blips_, which are our new estimation targets in a
categorical setting. We define pseudo-blips as vector-valued entities where the
output for a given $V$ is a vector of length equal to the number of treatment
categories, $n_A$. As such, we define it as:
$$\bar{Q}_0^{pblip}(V) = \{\bar{Q}_{0,a}^{pblip}(V): a \in \mathcal{A} \}$$

We implement three different pseudo-blips in `tmle3mopttx`.

1. _Blip1_ corresponds to choosing a reference category of treatment, and
   defining the blip for all other categories relative to the specified
   reference. Hence we have that:
   $$\bar{Q}_{0,a}^{pblip-ref}(V) \equiv E_0(Y_a-Y_0|V)$$ where $Y_0$ is the
   specified reference category with $A=0$. Note that, for the case of binary
   treatment, this strategy reduces to the approach described for the binary
   setup.

2. _Blip2_ approach corresponds to defining the blip relative to the average of
   all categories. As such, we can define $\bar{Q}_{0,a}^{pblip-avg}(V)$ as:
   $$\bar{Q}_{0,a}^{pblip-avg}(V) \equiv E_0(Y_a- \frac{1}{n_A} \sum_{a \in
     \mathcal{A}} Y_a|V)$$
   In the case where subject-matter knowledge regarding which reference category
   to use is not available, blip2 might be a viable option.

3. _Blip3_ reflects an extension of Blip2, where the average is now a weighted
   average:
   $$\bar{Q}_{0,a}^{pblip-wavg}(V) \equiv E_0(Y_a- \frac{1}{n_A} \sum_{a \in
     \mathcal{A}} Y_{a} P(A=a|V) |V)$$

Just like in the binary case, pseudo-blips are estimated by regressing contrasts
composed using the A-IPW transform on $V$.

### Note on Inference

In a randomized trial, statistical inference relies on the second-order
difference between the estimator of the optimal individualized treatment and the
optimal individualized treatment itself to be asymptotically negligible. This is
a reasonable condition if we consider rules that depend on small number of
covariates, or if we are willing to make smoothness assumptions. Alternatively,
we can consider TMLEs and statistical inference for data-adaptive target
parameters defined in terms of an estimate of the optimal individualized
treatment. In particular, instead of trying to estimate the mean under the true
optimal individualized treatment, we aim to estimate the mean under the
estimated optimal individualized treatment. As such, we develop cross-validated
TMLE approach that provides asymptotic inference under minimal conditions for
the mean under the estimate of the optimal individualized treatment. In
particular, considering the data adaptive parameter allows us to avoid
consistency and rate condition for the fitted optimal rule, as required for
asymptotic linearity of the TMLE of the mean under the actual, true optimal
rule. Practically, the estimated (data-adaptive) rule should be preferred, as
this possibly sub-optimal rule is the one implemented in the population.

### Why CV-TMLE?

As discussed in @vanderLaanLuedtke15, CV-TMLE is necessary as the
non-cross-validated TMLE is biased upward for the mean outcome under the rule,
and therefore overly optimistic. More generally however, using CV-TMLE allows us
more freedom in estimation and therefore greater data adaptivity, without
sacrificing inference.

## Interpreting the Causal Effect of an Optimal Individualized Intervention

In summary, the mean outcome under the optimal individualized treatment is a
counterfactual quantity of interest representing what the mean outcome would
have been if everybody, contrary to the fact, received treatment that optimized
their outcome. The optimal individualized treatment regime is a rule that
optimizes the mean outcome under the dynamic treatment, where the candidate
rules are restricted to only respond to a user-supplied subset of the baseline
and intermediate covariates. In essence, our target parameter answers the key
aim of precision medicine: allocating the available treatment by tailoring it to
the individual characteristics of the patient, with the goal of optimizing the
final outcome.

## Evaluating the Causal Effect of an OIT with Binary Treatment

Finally, we demonstrate how to evaluate the mean outcome under the optimal
individualized treatment using `tmle3mopptx`. To start, let's load the packages
we'll use and set a seed:

```{r setup-mopttx, message=FALSE, warning=FALSE}
library(here)
library(data.table)
library(sl3)
library(tmle3)
library(tmle3mopttx)
library(devtools)
set.seed(111)
```

### Simulated Data

First, we load the simulated data. We will start with the more general setup
where the treatment is a binary variable; later in the chapter we will consider
another data-generating distribution where $A$ is categorical. In this example,
our data generating distribution is of the following form:
\begin{align*}
  W &\sim \mathcal{N}(\bf{0},I_{3 \times 3})\\
  P(A=1|W) &= \frac{1}{1+\exp^{(-0.8*W_1)}}\\
  P(Y=1|A,W) &= 0.5\text{logit}^{-1}[-5I(A=1)(W_1-0.5)+5I(A=0)(W_1-0.5)] +
     0.5\text{logit}^{-1}(W_2W_3)
\end{align*}

```{r load sim_bin_data}
data("data_bin")
```

The above composes our observed data structure $O = (W, A, Y)$. Note that the
mean under the true optimal rule is $\psi=0.578$ for this data generating
distribution.

To formally express this fact using the `tlverse` grammar introduced by the
`tmle3` package, we create a single data object and specify the functional
relationships between the nodes in the _directed acyclic graph_ (DAG) via
_nonparametric structural equation models_ (NPSEMs), reflected in the node list
that we set up:

```{r data_nodes2-mopttx}
# organize data and nodes for tmle3
data <- data_bin
node_list <- list(
  W = c("W1", "W2", "W3"),
  A = "A",
  Y = "Y"
)
```

We now have an observed data structure (`data`) and a specification of the role
that each variable in the data set plays as the nodes in a DAG.

### Constructing Optimal Stacked Regressions with `sl3`

To easily incorporate ensemble machine learning into the estimation procedure,
we rely on the facilities provided in the [`sl3` R
package](https://tlverse.org/sl3). Using the framework provided by the [`sl3`
package](https://tlverse.org/sl3), the nuisance parameters of the TML estimator
may be fit with ensemble learning, using the cross-validation framework of the
Super Learner algorithm of @vdl2007super.

```{r mopttx_sl3_lrnrs2}
# Define sl3 library and metalearners:
lrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)
lrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)
lrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)
lrn_mean <- Lrnr_mean$new()
lrn_glm <- Lrnr_glm_fast$new()

## Define the Q learner:
Q_learner <- Lrnr_sl$new(
  learners = list(
    lrn_xgboost_50, lrn_xgboost_100,
    lrn_xgboost_500, lrn_mean, lrn_glm
  ),
  metalearner = Lrnr_nnls$new()
)

## Define the g learner:
g_learner <- Lrnr_sl$new(
  learners = list(lrn_xgboost_100, lrn_glm),
  metalearner = Lrnr_nnls$new()
)

## Define the B learner:
b_learner <- Lrnr_sl$new(
  learners = list(
    lrn_xgboost_50, lrn_xgboost_100,
    lrn_xgboost_500, lrn_mean, lrn_glm
  ),
  metalearner = Lrnr_nnls$new()
)
```

As seen above, we generate three different ensemble learners that must be fit,
corresponding to the learners for the outcome regression (Q), propensity score
(g), and the blip function (B). We make the above explicit with respect to
standard notation by bundling the ensemble learners into a list object below:

```{r mopttx_make_lrnr_list}
# specify outcome and treatment regressions and create learner list
learner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)
```

The `learner_list` object above specifies the role that each of the ensemble
learners we've generated is to play in computing initial estimators. Recall that
we need initial estimators of relevant parts of the likelihood in order to
building a TMLE for the parameter of interest. In particular, `learner_list`
makes explicit the fact that our `Y` is used in fitting the outcome regression,
while `A` is used in fitting the treatment mechanism regression, and finally `B`
is used in fitting the blip function.

### Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects

To start, we will initialize a specification for the TMLE of our parameter of
interest simply by calling `tmle3_mopttx_blip_revere`. We specify the argument
`V = c("W1", "W2", "W3")` when initializing the `tmle3_Spec` object in order to
communicate that we're interested in learning a rule dependent on `V`
covariates. Note that we don't have to specify `V`- this will result in a rule
that is not based on any collected covariates. We also need to specify the type
of pseudo-blip we will use in this estimation problem, the list of learners used
to estimate the blip function, whether we want to maximize or minimize the final
outcome, and few other more advanced features including searching for a less
complex rule and realistic interventions.

```{r mopttx_spec_init_complex}
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("W1", "W2", "W3"), type = "blip1",
  learners = learner_list,
  maximize = TRUE, complex = TRUE, realistic = FALSE
)
```

As seen above, the `tmle3_mopttx_blip_revere` specification object
(like all `tmle3_Spec` objects) does _not_ store the data for our
specific analysis of interest. Later,
we'll see that passing a data object directly to the `tmle3` wrapper function,
alongside the instantiated `tmle_spec`, will serve to construct a `tmle3_Task`
object internally.

We elaborate more on the initialization specifications. In initializing the
specification for the TMLE of our parameter of interest, we have specified the
set of covariates the rule depends on (`V`), the type of pseudo-blip to use
(`type`), and the learners used for estimating the relevant parts of the
likelihood and the blip function. In addition, we need to specify whether we
want to maximize the mean outcome under the rule (`maximize`), and whether we
want to estimate the rule under all the covariates $V$ provided by the user
(`complex`). If `FALSE`, `tmle3mopttx` will instead consider all the possible
rules under a smaller set of covariates including the static rules, and optimize
the mean outcome over all the subsets of $V$. As such, while the user might have
provided a full set of collected covariates as input for $V$, it is possible
that the true rule only depends on a subset of the set provided by the user. In
that case, our returned mean under the optimal individualized rule will be based
on the smaller subset. In addition, we provide an option to search for realistic
optimal individualized interventions via the `realistic` specification. If
`TRUE`, only treatments supported by the data will be considered, therefore
alleviating concerns regarding practical positivity issues. We explore all the
important extensions of `tmle3mopttx` in later sections.

```{r mopttx_fit_tmle_auto_blip_revere_complex, eval=T}
# fit the TML estimator
fit <- tmle3(tmle_spec, data, node_list, learner_list)
fit
```

We can see that the estimate of $psi_0$ is $0.56$, and that the confidence
interval covers our true mean under the true optimal individualized treatment.

## Evaluating the Causal Effect of an optimal ITR with Categorical Treatment

In this section, we consider how to evaluate the mean outcome under the optimal
individualized treatment when $A$ has more than two categories. While the
procedure is analogous to the previously described binary treatment, we now need
to pay attention to the type of blip we define in the estimation stage, as well
as how we construct our learners.

### Simulated Data

First, we load the simulated data. Here, our data generating distribution was
of the following form:
\begin{align*}
  W &\sim \mathcal{N}(\bf{0},I_{4 \times 4})\\
  P(A=a|W) &= \frac{1}{1+\exp^{(-0.8*W_1)}}\\
  P(Y=1|A,W) = 0.5\text{logit}^{-1}[15I(A=1)(W_1-0.5) - 3I(A=2)(2W_1+0.5) +
    3I(A=3)(3W_1-0.5)] +\text{logit}^{-1}(W_2W_1)
\end{align*}

We can just load the data available as part of the package as follows:

```{r load sim_cat_data}
data("data_cat_realistic")
```

The above composes our observed data structure $O = (W, A, Y)$. Note that the
mean under the true optimal rule is $\psi=0.658$, which is the quantity we aim
to estimate.

```{r data_nodes-mopttx}
# organize data and nodes for tmle3
data <- data_cat_realistic
node_list <- list(
  W = c("W1", "W2", "W3", "W4"),
  A = "A",
  Y = "Y"
)
```

### Constructing Optimal Stacked Regressions with `sl3`

```{r sl3_lrnrs-mopttx}
# Initialize few of the learners:
lrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)
lrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)
lrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)
lrn_mean <- Lrnr_mean$new()
lrn_glm <- Lrnr_glm_fast$new()

## Define the Q learner, which is just a regular learner:
Q_learner <- Lrnr_sl$new(
  learners = list(
    lrn_xgboost_50, lrn_xgboost_100, lrn_xgboost_500, lrn_mean,
    lrn_glm
  ),
  metalearner = Lrnr_nnls$new()
)

# Define the g learner, which is a multinomial learner:
# specify the appropriate loss of the multinomial learner:
mn_metalearner <- make_learner(Lrnr_solnp,
  loss_function = loss_loglik_multinomial,
  learner_function =
    metalearner_linear_multinomial
)
g_learner <- make_learner(
  Lrnr_sl,
  list(lrn_xgboost_100, lrn_xgboost_500, lrn_mean),
  mn_metalearner
)

# Define the Blip learner, which is a multivariate learner:
learners <- list(
  lrn_xgboost_50, lrn_xgboost_100, lrn_xgboost_500, lrn_mean,
  lrn_glm
)
b_learner <- create_mv_learners(learners = learners)
```

As seen above, we generate three different ensemble learners that must be fit,
corresponding to the learners for the outcome regression, propensity score, and
the blip function. Note that we need to estimate $g_0(A|W)$ for a categorical
$A$ -- therefore, we use the multinomial Super Learner option available within
the `sl3` package with learners that can address multi-class classification
problems. In order to see which learners can be used to estimate $g_0(A|W)$ in
`sl3`, we run the following:

```{r cat_learners}
# See which learners support multi-class classification:
sl3_list_learners(c("categorical"))
```

Note that since the corresponding blip will be vector valued, we will have a
column for each additional level of treatment. As such, we need to create
multivariate learners with the helper function `create_mv_learners` that takes a
list of initialized learners as input.

We make the above explicit with respect to standard notation by bundling the
ensemble learners into a list object below:

```{r make_lrnr_list-mopttx}
# specify outcome and treatment regressions and create learner list
learner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)
```

### Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects

```{r spec_init}
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("W1", "W2", "W3", "W4"), type = "blip2",
  learners = learner_list, maximize = TRUE, complex = TRUE,
  realistic = FALSE
)
```

```{r fit_tmle_auto, eval=T}
# fit the TML estimator
fit <- tmle3(tmle_spec, data, node_list, learner_list)

fit
```

We can see that the estimate of $psi_0$ is $0.60$, and that the confidence
interval covers our true mean under the true optimal individualized treatment.

## Extensions to Causal Effect of an OIT

In this section, we consider two extensions to the procedure described for
estimating the value of the OIT. First one considers a setting where the user
might be interested in a grid of possible sub-optimal rules, corresponding to
potentially limited knowledge of potential effect modifiers. The second
extension concerns implementation of a realistic optimal individual
interventions where certain regimes might be preferred, but due to practical or
global positivity restraints are not realistic to implement.

### Simpler Rules

In order to not only consider the most ambitious fully $V$-optimal rule, we
define $S$-optimal rules as the optimal rule that considers all possible subsets
of $V$ covariates, with card($S$) $\leq$ card($V$) and $\emptyset \in S$. This
allows us to consider sub-optimal rules that are easier to estimate and
potentially provide more realistic rules- as such, we allow for statistical
inference for the counterfactual mean outcome under the sub-optimal rule.
Within the `tmle3mopttx` paradigm, we just need to change the `complex`
parameter to `FALSE`:

```{r mopttx_spec_init_noncomplex}
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("W4", "W3", "W2", "W1"), type = "blip2",
  learners = learner_list,
  maximize = TRUE, complex = FALSE, realistic = FALSE
)
```

```{r mopttx_fit_tmle_auto_blip_revere_noncomplex, eval=T}
# fit the TML estimator
fit <- tmle3(tmle_spec, data, node_list, learner_list)

fit
```

Therefore even though the user specified all baseline covariates as the basis
for rule estimation, a simpler rule based on only $W_2$ and $W_1$ is sufficient
to maximize the mean under the optimal individualized treatment.

### Realistic Optimal Individual Regimes

In addition to considering less complex rules, `tmle3mopttx` also provides an
option to estimate the mean under the realistic, or implementable, optimal
individualized treatment. It is often the case that assigning particular regime
might have the ability to fully maximize (or minimize) the desired outcome, but
due to global or practical positivity constrains, such treatment can never be
implemented in real life (or is highly unlikely). As such, specifying
`realistic` to `TRUE`, we consider possibly suboptimal treatments that optimize
the outcome in question while being supported by the data.

```{r mopttx_spec_init_realistic}
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("W4", "W3", "W2", "W1"), type = "blip2",
  learners = learner_list,
  maximize = TRUE, complex = TRUE, realistic = TRUE
)
```

```{r mopttx_fit_tmle_auto_blip_revere_realistic, eval=T}
# fit the TML estimator
fit <- tmle3(tmle_spec, data, node_list, learner_list)
fit

# How many individuals got assigned each treatment?
table(tmle_spec$return_rule)
```

### Q-learning

Alternatively, we could estimate the mean under the optimal individualized
treatment using Q-learning. The optimal rule can be learned through fitting the
likelihood, and consequently estimating the optimal rule under this fit of the
likelihood [@Sutton1998, @murphy2003].

Below we outline how to use `tmle3mopttx` package in order to estimate the mean
under the ITR using Q-learning. As demonstrated in the previous sections, we
first need to initialize a specification for the TMLE of our parameter of
interest. As opposed to the previous section however, we will now use
`tmle3_mopttx_Q` instead of `tmle3_mopttx_blip_revere` in order to indicate that
we want to use Q-learning instead of TMLE.

```{r spec_init_Qlearning2}
# initialize a tmle specification
tmle_spec_Q <- tmle3_mopttx_Q(maximize = TRUE)

# Define data:
tmle_task <- tmle_spec_Q$make_tmle_task(data, node_list)

# Define likelihood:
initial_likelihood <- tmle_spec_Q$make_initial_likelihood(
  tmle_task,
  learner_list
)

# Estimate the parameter:
Q_learning(tmle_spec_Q, initial_likelihood, tmle_task)[1]
```

## Variable Importance Analysis with OIT

Suppose one wishes to assess the importance of each observed covariate, in
terms of maximizing (or minimizing) the population mean of an outcome under an
optimal individualized treatment regime. In particular, a covariate that
maximizes (or minimizes) the population mean outcome the most under an optimal
individualized treatment out of all other considered covariates under optimal
assignment might be considered _more important_ for the outcome. To put it in
context, perhaps optimal allocation of treatment 1, denoted $A_1$, results in a
larger mean outcome than optimal allocation of another treatment ($A_2$).
Therefore, we would label $A_1$ as having a higher variable importance with
regard to maximizing (minimizing) the mean outcome under the optimal
individualized treatment.

### Simulated Data

In order to run `tmle3mopttx` variable importance measure, we need to consider
covariates to be categorical variables. For illustration purpose, we bin
baseline covariates corresponding to the data-generating distribution described
in section 5.7.1:

```{r data_vim-nodes-mopttx}
# bin baseline covariates to 3 categories:
data$W1 <- ifelse(data$W1 < quantile(data$W1)[2], 1,
  ifelse(data$W1 < quantile(data$W1)[3], 2, 3)
)

node_list <- list(
  W = c("W3", "W4", "W2"),
  A = c("W1", "A"),
  Y = "Y"
)
```

Note that our node list now includes $W_1$ as treatments as well! Don't worry,
we will still properly adjust for all baseline covariates.

### Variable Importance using Targeted Estimation of the value of the ITR

In the previous sections we have seen how to obtain a contrast between the mean
under the optimal individualized rule and the mean under the observed outcome
for a single covariate- we are now ready to run the variable importance analysis
for all of our specified covariates. In order to run the variable importance
analysis, we first need to initialize a specification for the TMLE of our
parameter of interest as we have done before. In addition, we need to specify
the data and the corresponding list of nodes, as well as the appropriate
learners for the outcome regression, propensity score, and the blip function.
Finally, we need to specify whether we should adjust for all the other
covariates we are assessing variable importance for. We will adjust for all $W$s
in our analysis, and if `adjust_for_other_A=TRUE`, also for all $A$ covariates
that are not treated as exposure in the variable importance loop.

To start, we will initialize a specification for the TMLE of our parameter of
interest (called a `tmle3_Spec` in the `tlverse` nomenclature) simply by calling
`tmle3_mopttx_vim`. First, we indicate the method used for learning the optimal
individualized treatment by specifying the `method` argument of
`tmle3_mopttx_vim`. If `method="Q"`, then we will be using Q-learning for rule
estimation, and we do not need to specify `V`, `type` and `learners` arguments
in the spec, since they are not important for Q-learning. However, if
`method="SL"`, which corresponds to learning the optimal individualized
treatment using the above outlined methodology, then we need to specify the type
of pseudo-blip we will use in this estimation problem, whether we want to
maximize or minimize the outcome, complex and realistic rules. Finally, for
`method="SL"` we also need to communicate that we're interested in learning a
rule dependent on `V` covariates by specifying the `V` argument. For both
`method="Q"` and `method="SL"`, we need to indicate whether we want to maximize
or minimize the mean under the optimal individualized rule. Finally, we also
need to specify whether the final comparison of the mean under the optimal
individualized rule and the mean under the observed outcome should be on the
multiplicative scale (risk ratio) or linear (similar to average treatment
effect).

```{r mopttx_spec_init_vim}
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_vim(
  V = c("W2"),
  type = "blip2",
  learners = learner_list,
  contrast = "multiplicative",
  maximize = FALSE,
  method = "SL",
  complex = TRUE,
  realistic = FALSE
)
```

```{r mopttx_fit_tmle_auto_vim, eval=TRUE}
# fit the TML estimator
vim_results <- tmle3_vim(tmle_spec, data, node_list, learner_list,
  adjust_for_other_A = TRUE
)

print(vim_results)
```

The final result of `tmle3_vim` with the `tmle3mopttx` spec is an ordered list
of mean outcomes under the optimal individualized treatment for all categorical
covariates in our dataset.

## Real World Data and `tmle3mopttx`

Finally, we cement everything we learned so far with a real data application.
As in the previous sections, we will be using the WASH Benefits data,
corresponding to the Effect of water quality, sanitation, hand washing, and
nutritional interventions on child development in rural Bangladesh trial.
The main aim of the cluster-randomized controlled trial was to assess the
impact of six intervention groups, including:

1. chlorinated drinking water

2. improved sanitation

3. handwashing with soap

4. combined water, sanitation, and handwashing

5. improved nutrition through counselling and provision of lipid-based nutrient
   supplements

6. combined water, sanitation, handwashing, and nutrition.

We aim to estimate the optimal ITR and the corresponding value under the optimal
ITR for the main intervention in WASH Benefits data.

To start, let's load the data, convert all columns to be of class `numeric`,
and take a quick look at it:

```{r load-washb-data, message=FALSE, warning=FALSE, cache=FALSE}
washb_data <- fread("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv",
  stringsAsFactors = TRUE
)
washb_data <- washb_data[!is.na(momage), lapply(.SD, as.numeric)]
head(washb_data, 3)
```

As before, we specify the NPSEM via the `node_list` object. Our outcome of
interest is the weight-for-height Z-score which we seek to maximize, whereas our
treatment is the six intervention groups aimed at improving living conditions.
All the other collected baseline covariates correspond to $W$.

```{r washb-data-npsem-mopttx, message=FALSE, warning=FALSE, cache=FALSE}
node_list <- list(
  W = names(washb_data)[!(names(washb_data) %in%
    c("whz", "tr"))],
  A = "tr", Y = "whz"
)
```

We pick few potential effect modifiers, including mother's education, current
living conditions (floor), and possession of material items including the
refrigerator. We concentrate of these covariates as they might be indicative of
the socio-economic status of individuals involved in the trial.

```{r summary_WASH}
table(washb_data$momedu)
table(washb_data$floor)
table(washb_data$asset_refrig)
summary(washb_data$whz)
```

```{r sl3_lrnrs-WASH-mopttx, eval=FALSE}
# Initialize few of the learners:
lrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)
lrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)
lrn_mean <- Lrnr_mean$new()

## Define the Q learner, which is just a regular learner:
Q_learner <- Lrnr_sl$new(
  learners = list(lrn_xgboost_50, lrn_xgboost_100, lrn_mean),
  metalearner = Lrnr_nnls$new()
)

# Define the g learner, which is a multinomial learner:
# specify the appropriate loss of the multinomial learner:
mn_metalearner <- make_learner(Lrnr_solnp,
  loss_function = loss_loglik_multinomial,
  learner_function =
    metalearner_linear_multinomial
)
g_learner <- make_learner(
  Lrnr_sl,
  list(lrn_xgboost_100, lrn_mean),
  mn_metalearner
)

# Define the Blip learner, which is a multivariate learner:
learners <- list(lrn_xgboost_50, lrn_xgboost_100, lrn_mean)
b_learner <- create_mv_learners(learners = learners)

learner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)
```

```{r spec_init_WASH, eval=FALSE}
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("momedu", "floor", "asset_refrig"), type = "blip2",
  learners = learner_list, maximize = TRUE, complex = TRUE,
  realistic = FALSE
)

# fit the TML estimator
fit <- tmle3(tmle_spec, data = washb_data, node_list, learner_list)
fit
```

---

## Exercises

### Review of Key Concepts

1. What is the difference between dynamic and optimal individualized regimes?

2. What's the intuition behind using different blip types? Why did we switch
   from `blip1` to `blip2` when considering categorical treatment? What are some
   of the advantages of each?

3. Look back at the results generated in section 5.7.1, and compare then to the
   mean under the optimal individualized treatment in section 5.6. Why do you
   think the estimate if higher under the less complex rule? How does the set of
   covariates picked by `tmle3mopttx` compare to the baseline covariates the
   true rule depends on?

4. Compare the distribution of treatments assigned under the true optimal
   individualized treatment (section 5.6) and realistic optimal individualized
   treatment (section 5.7.2). Referring back to the data-generating
   distribution, why do you think the distribution of allocated treatment
   changed?

5. Using the same simulation, perform a variable importance analysis using
   Q-learning. How do the results change and why?

### The Ideas in Action

1. Using the WASH benefits data, extract the optimal ITR for exact individual.
   Which intervention is the most dominant? Why do you think that is?

2. Consider simpler rules for the WASH benefits data example. What set of rules
   are picked?

3. Using the WASH benefits data, estimate the realistic optimal ITR and the
   corresponding value of the realistic ITR. Did the results change?

4. Change the treatment to Mother's education (momedu), and estimate the
   value under the ITR in this setting. What do the results indicate? Can
   we intervene on such a variable?

### Advanced Topics

1. How can we extend the current approach to include exceptional laws?

2. How can we extend the current approach to incorporate resource constraints?

3. How can we extend the current approach to continuous interventions?

<!--
## Appendix

### Exercise solutions
-->

<!--chapter:end:07-tmle3mopttx.Rmd-->

# Stochastic Treatment Regimes

_Nima Hejazi_

Based on the [`tmle3shift` `R` package](https://github.com/tlverse/tmle3shift)
by _Nima Hejazi, Jeremy Coyle, and Mark van der Laan_.

Updated: `r Sys.Date()`

## Learning Objectives

1. Differentiate stochastic treatment regimes from static, dynamic, and optimal
   treatment regimes.
2. Describe how estimating causal effects of stochastic interventions informs a
   real-world data analysis.
3. Contrast a population level stochastic intervention policy from a modified
   treatment policy.
4. Estimate causal effects under stochastic treatment regimes with the
   `tmle3shift` `R` package.
6. Specify a grid of counterfactual shift interventions to be used for defining
   a set of stochastic intervention policies.
7. Interpret a set of effect estimates from a grid of counterfactual shift
   interventions.
5. Construct marginal structural models to measure variable importance in terms
   of stochastic interventions, using a grid of shift interventions.
8. Implement a shift intervention at the individual level, to facilitate
   shifting each individual to a value that's supported by the data.
9. Define novel shift intervention functions to extend the `tmle3shift` `R`
   package.

## Introduction to Stochastic Interventions

Stochastic treatment regimes present a relatively simple, yet extremely flexible
manner by which _realistic_ causal effects (and contrasts thereof) may be
defined. Importantly, stochastic treatment regimes may be applied to nearly
any manner of treatment variable -- continuous, ordinal, categorical, binary --
allowing for a rich set of causal effects to be defined through this formalism.
In this chapter, we examine a simple example of stochastic treatment regimes in
the context of a continuous treatment variable of interest, defining an
intuitive causal effect through which to examine stochastic interventions more
generally. In later sections, we introduce numerous extensions based on this
broad class of interventions -- from stochastic interventions on binary
treatment variables to stochastic mediation effects and data-adaptive inference
for stochastic intervention effects. As a first step to using stochastic
treatment regimes in practice, we present the [`tmle3shift` R
package](https://github.com/tlverse/tmle3shift), which features an
implementation of a recently developed algorithm for computing targeted minimum
loss-based estimates of a causal effect based on a stochastic treatment regime
that shifts the natural value of the treatment based on a shifting function
$d(A,W)$. For a comprehensive technical presentation of some of the material in
this chapter, the interested reader is invited to consult @diaz2018stochastic.
Additional background on the field of Targeted Learning, as well as prior work
on stochastic treatment regimes, is available in @vdl2011targeted,
@vdl2018targeted, and @diaz2012population.

While stochastic treatment regimes are arguably the most general of the
classes of interventions through which causal effects may be defined, such
interventions are conceptually simple.

## Data Structure and Notation

Consider $n$ observed units $O_1, \ldots, O_n$, where each random variable $O =
(W, A, Y)$ corresponds to a single observational unit. Let $W$ denote baseline
covariates (e.g., age, sex, education level), $A$ an intervention variable of
interest (e.g., nutritional supplements), and $Y$ an outcome of interest (e.g.,
disease status). Though it need not be the case, let $A$ be continuous-valued,
i.e. $A \in \mathbb{R}$. Let $O_i \sim \mathcal{P} \in \mathcal{M}$, where
$\mathcal{M}$ is the nonparametric statistical model defined as the set of
continuous densities on $O$ with respect to some dominating measure. To
formalize the definition of stochastic interventions and their corresponding
causal effects, we introduce a nonparametric structural equation model (NPSEM),
based on @pearl2009causality, to define how the system changes under posited
interventions:
\begin{align*}\label{eqn:npsem}
  W &= f_W(U_W) \\ A &= f_A(W, U_A) \\ Y &= f_Y(A, W, U_Y),
\end{align*}
where the set of structural equations provide a mechanistic model by which the
observed data $O$ is assumed to have been generated. There are several standard
assumptions embedded in the NPSEM -- specifically, a temporal ordering that
supposes that $Y$ occurs after $A$, which occurs after $W$; each variable
(i.e., $\{W, A, Y\}$) is assumed to have been generated from its corresponding
deterministic function (i.e., $\{f_W, f_A, f_Y\}$) of the observed variables
that precede it temporally, as well as an exogenous variable, denoted by $U$;
lastly, each exogenous variable is assumed to contain all unobserved causes of
the corresponding observed variable.

The likelihood of the data $O$ admits a factorization, wherein, for $p_0^O$,
the density of $O$ with respect to the product measure, the density evaluated
on a particular observation $o$ may be a written
\begin{equation*}\label{eqn:likelihood_factorization}
  p_0^O(x) = q^O_{0,Y}(y \mid A = a, W = w) q^O_{0,A}(a \mid W = w)
  q^O_{0,W}(w),
\end{equation*}
where $q_{0, Y}$ is the conditional density of $Y$ given $(A, W)$ with respect
to some dominating measure, $q_{0, A}$ is the conditional density of $A$ given
$W$ with respect to dominating measure $\mu$, and $q_{0, W}$ is the density of
$W$ with respect to dominating measure $\nu$. Further, for ease of notation,
let $Q(A, W) = \mathbb{E}[Y \mid A, W]$, $g(A \mid W) = \mathbb{P}(A \mid W)$,
and $q_W$ the marginal distribution of $W$. These components of the likelihood
will be essential in developing an understanding of the manner in which
stochastic treatment regimes pertrub a system and how a corresponding causal
effect may be evaluated. Importantly, the NPSEM parameterizes $p_0^O$ in terms
of the distribution of random variables $(O, U)$ modeled by the system of
equations. In turn, this implies a model for the distribution of counterfactual
random variables generated by interventions on the data-generating process.

## Defining the Causal Effect of a Stochastic Intervention

As causal effects are defined in terms of hypothetical interventions on the
NPSEM (\ref{eqn:npsem}), we may consider stochastic interventions in two
equivalent ways: (1) where the equation $f_A$, giving rise to $A$, is replaced
by a probabilistic mechanism $g_{\delta}(A \mid W)$ that differs from the
original $g(A \mid W)$, or (2) where the observed value $A$ is replaced by a
new value $A_{d(A,W)}$ based on applying a user-defined function $d(A,W)$ to
$A$. In the former case, the _stochastically modified_ value of the treatment
$A_{\delta}$ is drawn from a user-specified distribution $g_\delta(A \mid W)$,
which may depend on the original distribution $g(A \mid W)$ and is indexed by
a user-specified parameter $\delta$. In this case, the stochastically modified
value of the treatment $A_{\delta} \sim g_{\delta}(\cdot \mid W)$.
Alternatively, in the latter case, the stochastic treatment regime may be
viewed as an intervention in which $A$ is set equal to a value based on a
hypothetical regime $d(A, W)$, where regime $d$ depends on the treatment level
$A$ that would be assigned in the absence of the regime as well as the
covariates $W$. In either case, one may view the stochastic intervention as
generating a counterfactual random variable $Y_{d(A,W)} := f_Y(d(A,W), W, U_Y)
\equiv Y_{g_{\delta}} := f_Y(A_{\delta}, W, U_Y)$, where the counterfactual
outcome $Y_{d(A,W)} \sim \mathcal{P}_0^{\delta}$.

Stochastic interventions of this second variety may be referred to as depending
on the _natural value of treatment_ or as _modified treatment policies_.
@haneuse2013estimation and @young2014identification provide a discussion of the
critical differences and similarities in the identification and interpretation
of these two classes of stochastic intervention. In the sequel, we will
restrict our attention to a simple stochastic treatment regime that has been
characterized as a _modified treatment policy_ (MTP). Letting $A$ denote a
continuous-valued treatment, such as the taking of nutritional supplements
(e.g., number of vitamin pills) and assume that the distribution of $A$
conditional on $W = w$ has support in the interval $(l(w), u(w))$. That is, the
minimum observed number of pills taken $A$ for an individual with covariates
$W = w$ is $l(w)$; similarly, the maximum is $u(w)$. Then, a simple stochastic
intervention, based on a shift $\delta$, may be defined
\begin{equation}\label{eqn:shift}
  d(a, w) =
  \begin{cases}
    a - \delta & \text{if } a > l(w) + \delta \\
    a & \text{if } a \leq l(w) + \delta,
  \end{cases}
\end{equation}
where $0 \leq \delta \leq u(w)$ is an arbitrary pre-specified value that
defines the degree to which the observed value $A$ is to be shifted, where
possible. Such a stochastic treatment regime may be interpreted as the result
of a clinic policy that encourages individuals to consume $\delta$ more vitamin
pills than they would normally, i.e., based on their baseline characteristics.
The interpretation of this stochastic intervention may be made more interesting
by allowing the modification $\delta$ that it engenders to be a function of the
baseline covariates $W$, thereby allowing for the number of vitamin pills taken
to be a function of covariates such as age, sex, comorbidities, etc. This class
of stochastic interventions was first introduced by @diaz2012population and has
been further discussed in @haneuse2013estimation, @diaz2018stochastic, and
@hejazi2019+generally. Note that this intervention may be written in a manner
consistent with the first class of stochastic treatment regimes discussed as
well -- that is, as per @diaz2012population, $\mathbb{P}_{\delta}(g_0)(A = a
\mid W) = g_0(a - \delta(W) \mid W)$.

The goal of any causal analysis motivated by such a stochastic intervention is
to estimate a parameter defined as the counterfactual mean of the outcome with
respect to the stochastically modified intervention distribution. In
particular, the target causal estimand of our analysis is $\psi_{0, \delta} :=
\mathbb{E}_{P_0^{\delta}}\{Y_{d(A,W)}\}$, the mean of the counterfactual
outcome variable $Y_{d(A, W)}$. In prior work, @diaz2012population showed that
the causal quantity of interest $\mathbb{E}_0 \{Y_{d(A, W)}\}$ is identified by
a functional of the distribution of $O$:
\begin{align*}\label{eqn:identification2012}
  \psi_{0,d} = \int_{\mathcal{W}} \int_{\mathcal{A}} & \mathbb{E}_{P_0}
   \{Y \mid A = d(a, w), W = w\} \cdot \\ &q_{0, A}^O(a \mid W = w) \cdot
   q_{0, W}^O(w) d\mu(a)d\nu(w).
\end{align*}
If the identification conditions may be assumed to hold, then the statistical
parameter in \ref{eqn:identification2012} matches exactly the counterfactual
outcome $\psi_{0, \delta}$ under such an intervention, allowing for the causal
effect to be learned from the observed data $O$. @diaz2012population provide a
derivation based on the efficient influence function (EIF) in the nonparametric
model $\mathcal{M}$ and develop several estimators of this quantity, including
substitution, inverse probability weighted (IPW), augmented inverse probability
weighted (AIPW), and targeted maximum likelihood (TML) estimators, allowing for
semiparametric-efficient estimation and inference on the quantity of interest.
As per @diaz2018stochastic, the statistical target parameter may also be
denoted $\Psi(P_0) = \mathbb{E}_{P_0}{\overline{Q}(d(A, W), W)}$, where
$\overline{Q}(d(A, W), W)$ is the counterfactual outcome value of a given
individual under the stochastic intervention distribution.

Although the focus of this work is neither the establishment of identification
results nor the development of theoretical details, we review the necessary
identification details for the counterfactual mean under a stochastic
intervention here, in the interest of completeness. Paraphrasing from
@diaz2012population and @diaz2018stochastic, four standard assumptions are
necessary in order to establish identifiability of the causal parameter from
the observed data via the statistical functional -- these are

1. _Consistency_: $Y^{d(a_i, w_i)}_i = Y_i$ in the event $A_i = d(a_i, w_i)$,
   for $i = 1, \ldots, n$
2. _Stable unit value treatment assumption (SUTVA)_: $Y^{d(a_i, w_i)}_i$ does
   not depend on $d(a_j, w_j)$ for $i = 1, \ldots, n$ and $j \neq i$, or lack
   of interference [@rubin1978bayesian; @rubin1980randomization].
3. _Strong ignorability_: $A_i \indep Y^{d(a_i, w_i)}_i \mid W_i$, for $i = 1,
   \ldots, n$.
4. Positivity (or overlap)_: $a_i \in \mathcal{A} \implies d(a_i, w_i) \in
   \mathcal{A}$ for all $w \in \mathcal{W}$, where $\mathcal{A}$ denotes the
   support of $A \mid W = w_i \quad \forall i = 1, \ldots n$.

With the identification assumptions satisfied, @diaz2012population and
@diaz2018stochastic provide an efficient influence function with  respect to
the nonparametric model $\mathcal{M}$ as
\begin{equation*}\label{eqn:eif}
  D(P_0)(x) = H(a, w)({y - \overline{Q}(a, w)}) +
  \overline{Q}(d(a, w), w) - \Psi(P_0),
\end{equation*}
where the auxiliary covariate $H(a,w)$ may be expressed
\begin{equation*}\label{eqn:aux_covar_full}
  H(a,w) = \mathbb{I}(a + \delta < u(w)) \frac{g_0(a - \delta \mid w)}
  {g_0(a \mid w)} + \mathbb{I}(a + \delta \geq u(w)),
\end{equation*}
which may be reduced to
\begin{equation*}\label{eqn:aux_covar_simple}
  H(a,w) = \frac{g_0(a - \delta \mid w)}{g_0(a \mid w)} + 1
\end{equation*}
in the case that the treatment is within the limits that arise from conditioning
on $W$, i.e., for $A_i \in (u(w) - \delta, u(w))$.

The efficient influence function allows the construction of a
semiparametric-efficient estimators may be constructed. In the sequel, we focus
on a targeted maximum likelihood (TML) estimator, for which @diaz2018stochastic
give a recipe:

1. Construct initial estimators $g_n$ of $g_0(A, W)$ and $Q_n$ of
   $\overline{Q}_0(A, W)$, perhaps using data-adaptive regression techniques.
2. For each observation $i$, compute an estimate $H_n(a_i, w_i)$ of the
   auxiliary covariate $H(a_i,w_i)$.
3. Estimate the parameter $\epsilon$ in the logistic regression model
   $$ \text{logit}\overline{Q}_{\epsilon, n}(a, w) =
   \text{logit}\overline{Q}_n(a, w) + \epsilon H_n(a, w),$$
   or an alternative regression model incorporating weights.
4. Compute TML estimator $\Psi_n$ of the target parameter, defining update
   $\overline{Q}_n^{\star}$ of the initial estimate
   $\overline{Q}_{n, \epsilon_n}$:
   \begin{equation*}\label{eqn:tmle}
     \Psi_n = \Psi(P_n^{\star}) = \frac{1}{n} \sum_{i = 1}^n
     \overline{Q}_n^{\star}(d(A_i, W_i), W_i).
   \end{equation*}

## Interpreting the Causal Effect of a Stochastic Intervention

```{r, fig.cap="Animation of how a counterfactual outcome changes as the natural treatment distribution is subjected to a simple stochastic intervention", echo=FALSE, eval=TRUE, out.width='60%'}
knitr::include_graphics(path = "img/gif/shift_animation.gif")
```

## Evaluating the Causal Effect of a Stochastic Intervention

To start, let us load the packages we will use and set a seed for simulation:

```{r setup-shift, message=FALSE, warning=FALSE}
library(here)
library(tidyverse)
library(data.table)
library(sl3)
library(tmle3)
library(tmle3shift)
set.seed(429153)
```

We need to estimate two components of the likelihood in order to construct a
TML estimator. The first of these components is the outcome regression,
$\hat{Q}_n$, which is a simple regression of the form $\mathbb{E}[Y \mid A,W]$.
An estimate for such a quantity may be constructed using the Super Learner
algorithm. We construct the components of an `sl3`-style Super Learner for a
regression below, using a small variety of parametric and nonparametric
regression techniques:

```{r sl3_lrnrs-Qfit-shift, message=FALSE, warning=FALSE}
# learners used for conditional expectation regression
lrn_mean <- Lrnr_mean$new()
lrn_fglm <- Lrnr_glm_fast$new()
lrn_xgb <- Lrnr_xgboost$new(nrounds = 200)
sl_lrn <- Lrnr_sl$new(
  learners = list(lrn_mean, lrn_fglm, lrn_xgb),
  metalearner = Lrnr_nnls$new()
)
```

The second of these is an estimate of the treatment mechanism, $\hat{g}_n$,
i.e., the _propensity score_. In the case of a continuous intervention node
$A$, such a quantity takes the form $p(A \mid W)$, which is a conditional
density. Generally speaking, conditional density estimation is a challenging
problem that has received much attention in the literature. To estimate the
treatment mechanism, we must make use of learning algorithms specifically suited
to conditional density estimation; a list of such learners may be extracted from
`sl3` by using `sl3_list_learners()`:

```{r sl3_density_lrnrs_search-shift, message=FALSE, warning=FALSE}
sl3_list_learners("density")
```

To proceed, we'll select two of the above learners, `Lrnr_haldensify` for using
the highly adaptive lasso for conditional density estimation, based on an
algorithm given by @diaz2011super and implemented in @hejazi2019haldensify, and
`Lrnr_rfcde`, an approach for using random forests for conditional density
estimation [@pospisil2018rfcde]. A Super Learner may be constructed by pooling
estimates from each of these modified conditional density regression techniques.

```{r sl3_lrnrs-gfit-shift, message=FALSE, warning=FALSE}
# learners used for conditional density regression (i.e., propensity score)
lrn_haldensify <- Lrnr_haldensify$new(
  n_bins = 5, grid_type = "equal_mass",
  lambda_seq = exp(seq(-1, -13, length = 500))
)
lrn_rfcde <- Lrnr_rfcde$new(
  n_trees = 1000, node_size = 5,
  n_basis = 31, output_type = "observed"
)
sl_lrn_dens <- Lrnr_sl$new(
  learners = list(lrn_haldensify, lrn_rfcde),
  metalearner = Lrnr_solnp_density$new()
)
```

Finally, we construct a `learner_list` object for use in constructing a TML
estimator of our target parameter of interest:

```{r learner-list-shift, message=FALSE, warning=FALSE}
learner_list <- list(Y = sl_lrn, A = sl_lrn_dens)
```

The `learner_list` object above specifies the role that each of the ensemble
learners we have generated is to play in computing initial estimators to be
used in building a TMLE for the parameter of interest here. In particular, it
makes explicit the fact that our `Q_learner` is used in fitting the outcome
regression while our `g_learner` is used in estimating the treatment mechanism.

### Example with Simulated Data

```{r sim_data, message=FALSE, warning=FALSE}
# simulate simple data for tmle-shift sketch
n_obs <- 500 # number of observations
tx_mult <- 2 # multiplier for the effect of W = 1 on the treatment

## baseline covariates -- simple, binary
W <- replicate(2, rbinom(n_obs, 1, 0.5))

## create treatment based on baseline W
A <- rnorm(n_obs, mean = tx_mult * W, sd = 1)

## create outcome as a linear function of A, W + white noise
Y <- rbinom(n_obs, 1, prob = plogis(A + W))

# organize data and nodes for tmle3
data <- data.table(W, A, Y)
setnames(data, c("W1", "W2", "A", "Y"))
node_list <- list(W = c("W1", "W2"), A = "A", Y = "Y")
head(data)
```

The above composes our observed data structure $O = (W, A, Y)$. To formally
express this fact using the `tlverse` grammar introduced by the `tmle3` package,
we create a single data object and specify the functional relationships between
the nodes in the _directed acyclic graph_ (DAG) via _nonparametric structural
equation models_ (NPSEMs), reflected in the node list that we set up:

We now have an observed data structure (`data`) and a specification of the role
that each variable in the data set plays as the nodes in a DAG.

To start, we will initialize a specification for the TMLE of our parameter of
interest (called a `tmle3_Spec` in the `tlverse` nomenclature) simply by calling
`tmle_shift`. We specify the argument `shift_val = 0.5` when initializing the
`tmle3_Spec` object to communicate that we're interested in a shift of $0.5$ on
the scale of the treatment $A$ -- that is, we specify $\delta = 0.5$ (note that
this is an arbitrarily chosen value for this example).

```{r spec_init-shift, message=FALSE, warning=FALSE}
# initialize a tmle specification
tmle_spec <- tmle_shift(
  shift_val = 0.5,
  shift_fxn = shift_additive_bounded,
  shift_fxn_inv = shift_additive_bounded_inv
)
```

As seen above, the `tmle_shift` specification object (like all `tmle3_Spec`
objects) does _not_ store the data for our specific analysis of interest. Later,
we'll see that passing a data object directly to the `tmle3` wrapper function,
alongside the instantiated `tmle_spec`, will serve to construct a `tmle3_Task`
object internally (see the `tmle3` documentation for details).

### Targeted Estimation of Stochastic Interventions Effects

```{r fit_tmle-shift, message=FALSE, warning=FALSE, cache=FALSE}
tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)
tmle_fit
```

The `print` method of the resultant `tmle_fit` object conveniently displays the
results from computing our TML estimator.

### Statistical Inference for Targeted Maximum Likelihood Estimates

Recall that the asymptotic distribution of TML estimators has been studied
thoroughly:
$$\psi_n - \psi_0 = (P_n - P_0) \cdot D(\bar{Q}_n^*, g_n) + R(\hat{P}^*, P_0),$$
which, provided the following two conditions:

1. If $D(\bar{Q}_n^*, g_n)$ converges to $D(P_0)$ in $L_2(P_0)$ norm, and
2. the size of the class of functions considered for estimation of $\bar{Q}_n^*$
   and $g_n$ is bounded (technically, $\exists \mathcal{F}$ st
   $D(\bar{Q}_n^*, g_n) \in \mathcal{F}$ *__whp__*, where $\mathcal{F}$ is a
   Donsker class),
readily admits the conclusion that
$\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + R(\hat{P}^*, P_0)$.

Under the additional condition that the remainder term $R(\hat{P}^*, P_0)$
decays as $o_P \left( \frac{1}{\sqrt{n}} \right),$ we have that
$$\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + o_P \left( \frac{1}{\sqrt{n}}
 \right),$$
which, by a central limit theorem, establishes a Gaussian limiting distribution
for the estimator:

$$\sqrt{n}(\psi_n - \psi) \to N(0, V(D(P_0))),$$
where $V(D(P_0))$ is the variance of the efficient influence curve (canonical
gradient) when $\psi$ admits an asymptotically linear representation.

The above implies that $\psi_n$ is a $\sqrt{n}$-consistent estimator of $\psi$,
that it is asymptotically normal (as given above), and that it is locally
efficient. This allows us to build Wald-type confidence intervals in a
straightforward manner:

$$\psi_n \pm z_{\alpha} \cdot \frac{\sigma_n}{\sqrt{n}},$$
where $\sigma_n^2$ is an estimator of $V(D(P_0))$. The estimator $\sigma_n^2$
may be obtained using the bootstrap or computed directly via the following

$$\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^*, g_n)(O_i)$$

Having now re-examined these facts, let's simply examine the results of
computing our TML estimator:

## Extensions: Variable Importance Analysis with Stochastic Interventions

### Defining a grid of counterfactual interventions

In order to specify a _grid_ of shifts $\delta$ to be used in defining a set of
stochastic intervention policies in an _a priori_ manner, let us consider an
arbitrary scalar $\delta$ that defines a counterfactual outcome $\psi_n =
Q_n(d(A, W), W)$, where, for simplicity, let $d(A, W) = A + \delta$. A
simplified expression of the auxiliary covariate for the TMLE of $\psi$ is
$H_n = \frac{g^{\star}(a \mid w)}{g(a \mid w)}$, where $g^{\star}(a \mid w)$
defines the treatment mechanism with the stochastic intervention implemented.
Then, to ascertain whether a given choice of the shift $\delta$ is admissable
(in the sense of avoiding violations of the positivity assumption), let there
be a bound $C(\delta) = \frac{g^{\star}(a \mid w)}{g(a \mid w)} < M$, where
$g^{\star}(a \mid w)$ is a function of $\delta$ in part, and $M$ is a potentially
user-specified upper bound of $C(\delta)$. Then, $C(\delta)$ is a measure of
the influence of a given observation, thereby providing a way to limit the
maximum influence of a given observation (by way of the bound $M$ placed on
$C(\delta)$) through a choice of the shift $\delta$.

We formalize and extend the procedure to determine an acceptable set of values
for the shift $\delta$ in the sequel. Specifically, let there be a shift $d(A,
W) = A + \delta(A, W)$, where the shift $\delta(A, W)$ is defined as
\begin{equation}
  \delta(a, w) =
    \begin{cases}
      \delta, & \delta_{\text{min}}(a,w) \leq \delta \leq
        \delta_{\text{max}}(a,w) \\
      \delta_{\text{max}}(a,w), & \delta \geq \delta_{\text{max}}(a,w) \\
      \delta_{\text{min}}(a,w), & \delta \leq \delta_{\text{min}}(a,w) \\
    \end{cases},
\end{equation}
where $$\delta_{\text{max}}(a, w) = \text{argmax}_{\left\{\delta \geq 0,
\frac{g(a - \delta \mid w)}{g(a \mid w)} \leq M \right\}} \frac{g(a - \delta
\mid w)}{g(a \mid w)}$$ and
$$\delta_{\text{min}}(a, w) = \text{argmin}_{\left\{\delta \leq 0,
\frac{g(a - \delta \mid w)}{g(a \mid w)} \leq M \right\}} \frac{g(a - \delta
\mid w)}{g(a \mid w)}.$$

The above provides a strategy for implementing a shift at the level of a given
observation $(a_i, w_i)$, thereby allowing for all observations to be shifted
to an appropriate value -- whether $\delta_{\text{min}}$, $\delta$, or
$\delta_{\text{max}}$.

For the purpose of using such a shift in practice, the present software
provides the functions `shift_additive_bounded` and
`shift_additive_bounded_inv`, which define a variation of this shift:
\begin{equation}
  \delta(a, w) =
    \begin{cases}
      \delta, & C(\delta) \leq M \\
      0, \text{otherwise} \\
    \end{cases},
\end{equation}
which corresponds to an intervention in which the natural value of treatment
of a given observational unit is shifted by a value $\delta$ in the case that
the ratio of the intervened density $g^{\star}(a \mid w)$ to the natural
density $g(a \mid w)$ (that is, $C(\delta)$) does not exceed a bound $M$. In
the case that the ratio $C(\delta)$ exceeds the bound $M$, the stochastic
intervention policy does not apply to the given unit and they remain at their
natural value of treatment $a$.

### Initializing `vimshift` through its `tmle3_Spec`

To start, we will initialize a specification for the TMLE of our parameter of
interest (called a `tmle3_Spec` in the `tlverse` nomenclature) simply by calling
`tmle_shift`. We specify the argument `shift_grid = seq(-1, 1, by = 1)`
when initializing the `tmle3_Spec` object to communicate that we're interested
in assessing the mean counterfactual outcome over a grid of shifts `r seq(-1,
1, by = 1)` on the scale of the treatment $A$ (note that the numerical
choice of shift is an arbitrarily chosen set of values for this example).

```{r vim_spec_init, message=FALSE, warning=FALSE}
# what's the grid of shifts we wish to consider?
delta_grid <- seq(-1, 1, 1)

# initialize a tmle specification
tmle_spec <- tmle_vimshift_delta(
  shift_grid = delta_grid,
  max_shifted_ratio = 2
)
```

As seen above, the `tmle_vimshift` specification object (like all `tmle3_Spec`
objects) does _not_ store the data for our specific analysis of interest. Later,
we'll see that passing a data object directly to the `tmle3` wrapper function,
alongside the instantiated `tmle_spec`, will serve to construct a `tmle3_Task`
object internally (see the `tmle3` documentation for details).

### Targeted Estimation of Stochastic Interventions Effects

One may walk through the step-by-step procedure for  fitting the TML estimator
of the mean counterfactual outcome under each shift in the grid, using the
machinery exposed by the [`tmle3` R package](https://tmle3.tlverse.org/).

One may invoke the `tmle3` wrapper function (a user-facing convenience utility)
to fit the series of TML estimators (one for each parameter defined by the grid
delta) in a single function call:

```{r fit_tmle_wrapper_vimshift, message=FALSE, warning=FALSE, cache=FALSE}
tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)
tmle_fit
```

_Remark_: The `print` method of the resultant `tmle_fit` object conveniently
displays the results from computing our TML estimator.

### Inference with Marginal Structural Models

Since we consider estimating the mean counterfactual outcome $\psi_n$ under
several values of the intervention $\delta$, taken from the aforementioned
$\delta$-grid, one approach for obtaining inference on a single summary measure
of these estimated quantities involves leveraging working marginal structural
models (MSMs). Summarizing the estimates $\psi_n$ through a working MSM allows
for inference on the _trend_ imposed by a $\delta$-grid to be evaluated via a
simple hypothesis test on a parameter of this working MSM. Letting
$\psi_{\delta}(P_0)$ be the mean outcome under a shift $\delta$ of the
treatment, we have $\vec{\psi}_{\delta} = (\psi_{\delta}: \delta)$ with
corresponding estimators $\vec{\psi}_{n, \delta} = (\psi_{n, \delta}: \delta)$.
Further, let $\beta(\vec{\psi}_{\delta}) = \phi((\psi_{\delta}: \delta))$.

For a given MSM $m_{\beta}(\delta)$, we have that
$$\beta_0 = \text{argmin}_{\beta} \sum_{\delta}(\psi_{\delta}(P_0) -
m_{\beta}(\delta))^2 h(\delta),$$
which is the solution to
$$u(\beta, (\psi_{\delta}: \delta)) = \sum_{\delta}h(\delta)
\left(\psi_{\delta}(P_0) - m_{\beta}(\delta) \right) \frac{d}{d\beta}
m_{\beta}(\delta) = 0.$$
This then leads to the following expansion
$$\beta(\vec{\psi}_n) - \beta(\vec{\psi}_0) \approx -\frac{d}{d\beta} u(\beta_0,
\vec{\psi}_0)^{-1} \frac{d}{d\psi} u(\beta_0, \psi_0)(\vec{\psi}_n -
\vec{\psi}_0),$$
where we have
$$\frac{d}{d\beta} u(\beta, \psi) = -\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta)^t \frac{d}{d\beta} m_{\beta}(\delta)
-\sum_{\delta} h(\delta) m_{\beta}(\delta) \frac{d^2}{d\beta^2}
m_{\beta}(\delta),$$
which, in the case of an MSM that is a linear model (since
$\frac{d^2}{d\beta^2} m_{\beta}(\delta) = 0$), reduces simply to
$$\frac{d}{d\beta} u(\beta, \psi) = -\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta)^t \frac{d}{d\beta} m_{\beta}(\delta),$$
and
$$\frac{d}{d\psi}u(\beta, \psi)(\psi_n - \psi_0) = \sum_{\delta} h(\delta)
\frac{d}{d\beta} m_{\beta}(\delta) (\psi_n - \psi_0)(\delta),$$
which we may write in terms of the efficient influence function (EIF) of $\psi$
by using the first order approximation $(\psi_n - \psi_0)(\delta) =
\frac{1}{n}\sum_{i = 1}^n \text{EIF}_{\psi_{\delta}}(O_i)$,
where $\text{EIF}_{\psi_{\delta}}$ is the efficient influence function (EIF) of
$\vec{\psi}$.

Now, say, $\vec{\psi} = (\psi(\delta): \delta)$ is d-dimensional, then we may
write the efficient influence function of the MSM parameter $\beta$ as follows
$$\text{EIF}_{\beta}(O) = \left(\sum_{\delta} h(\delta) \frac{d}{d\beta}
m_{\beta}(\delta) \frac{d}{d\beta} m_{\beta}(\delta)^t \right)^{-1} \cdot
\sum_{\delta} h(\delta) \frac{d}{d\beta} m_{\beta}(\delta)
\text{EIF}_{\psi_{\delta}}(O),$$ where the first term is of dimension
$d \times d$ and the second term is of dimension $d \times 1$. In the above, we
assume a linear working MSM; however, an analogous procedure may be applied for
working MSMs based on GLMs.

Inference for a parameter of an MSM may be obtained by straightforward
application of the delta method (discussed previously) -- that is, we may
write the efficient influence function of the MSM parameter $\beta$ in terms of
the EIFs of each of the corresponding point estimates. Based on this, inference
from a working MSM is rather straightforward. To wit, the limiting distribution
for $m_{\beta}(\delta)$ may be expressed $$\sqrt{n}(\beta_n - \beta_0) \to N(0,
\Sigma),$$ where $\Sigma$ is the empirical covariance matrix of
$\text{EIF}_{\beta}(O)$.

```{r msm_fit, message=FALSE, warning=FALSE}
tmle_fit$summary[4:5, ]
```

#### Directly Targeting the MSM Parameter $\beta$

Note that in the above, a working MSM is fit to the individual TML estimates of
the mean counterfactual outcome under a given value of the shift $\delta$ in the
supplied grid. The parameter of interest $\beta$ of the MSM is asymptotically
linear (and, in fact, a TML estimator) as a consequence of its construction from
individual TML estimators. In smaller samples, it may be prudent to perform a
TML estimation procedure that targets the parameter $\beta$ directly, as opposed
to constructing it from several independently targeted TML estimates. An
approach for constructing such an estimator is proposed in the sequel.

Suppose a simple working MSM $\mathbb{E}Y_{g^0_{\delta}} = \beta_0 + \beta_1
\delta$, then a TML estimator targeting $\beta_0$ and $\beta_1$ may be
constructed as
$$\overline{Q}_{n, \epsilon}(A,W) = \overline{Q}_n(A,W) + \epsilon (H_1(g),
H_2(g),$$ for all $\delta$, where $H_1(g)$ is the auxiliary covariate for
$\beta_0$ and $H_2(g)$ is the auxiliary covariate for $\beta_1$.

To construct a targeted maximum likelihood estimator that directly targets the
parameters of the working marginal structural model, we may use the
`tmle_vimshift_msm` Spec (instead of the `tmle_vimshift_delta` Spec that
appears above):

```{r vim_targeted_msm_fit, message=FALSE, warning=FALSE, cache=FALSE}
# initialize a tmle specification
tmle_msm_spec <- tmle_vimshift_msm(
  shift_grid = delta_grid,
  max_shifted_ratio = 2
)

# fit the TML estimator and examine the results
tmle_msm_fit <- tmle3(tmle_msm_spec, data, node_list, learner_list)
tmle_msm_fit
```

### Example with the WASH Benefits Data

To complete our walk through, let's turn to using stochastic interventions to
investigate the data from the WASH Benefits trial. To start, let's load the
data, convert all columns to be of class `numeric`, and take a quick look at it

```{r load_washb_data_shift, message=FALSE, warning=FALSE, cache=FALSE}
washb_data <- fread("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv",
  stringsAsFactors = TRUE
)
washb_data <- washb_data[!is.na(momage), lapply(.SD, as.numeric)]
head(washb_data, 3)
```

Next, we specify our NPSEM via the `node_list` object. For our example analysis,
we'll consider the outcome to be the weight-for-height Z-score (as in previous
chapters), the intervention of interest to be the mother's age at time of
child's birth, and take all other covariates to be potential confounders.

```{r washb_data_npsem_shift, message=FALSE, warning=FALSE, cache=FALSE}
node_list <- list(
  W = names(washb_data)[!(names(washb_data) %in%
    c("whz", "momage"))],
  A = "momage", Y = "whz"
)
```

Were we to consider the counterfactual weight-for-height Z-score under shifts in
the age of the mother at child's birth, how would we interpret estimates of our
parameter? To simplify our interpretation, consider a shift of just a year in
the mother's age (i.e., $\delta = 1$); in this setting, a stochastic
intervention would correspond to a policy advocating that potential mothers
defer having a child for a single calendar year, possibly implemented through an
encouragement design deployed in a clinical setting.

For this example, we'll use the variable importance strategy of considering a
grid of stochastic interventions to evaluate the weight-for-height Z-score under
a shift in the mother's age down by two years ($\delta = -2$) or up by two years
($\delta = 2$). To do this, we simply initialize a `Spec` `tmle_vimshift_delta`
just as we did in a previous example:

```{r vim_spec_init_washb_shift, message=FALSE, warning=FALSE}
# initialize a tmle specification for the variable importance parameter
washb_vim_spec <- tmle_vimshift_delta(
  shift_grid = c(-2, 2),
  max_shifted_ratio = 2
)
```

Prior to running our analysis, we'll modify the `learner_list` object we had
created such that the density estimation procedure we rely on will be only the
random forest conditional density estimation procedure of @pospisil2018rfcde, as
the nonparametric conditional density approach based on the highly adaptive
lasso [@diaz2011super; @benkeser2016hal; @coyle2018hal9001;
@hejazi2019haldensify] is currently unable to accommodate large datasets.

```{r sl3_lrnrs_gfit_washb_shift, message=FALSE, warning=FALSE}
# learners used for conditional density regression (i.e., propensity score)
lrn_rfcde <- Lrnr_rfcde$new(
  n_trees = 250, node_size = 5,
  n_basis = 20, output_type = "observed"
)

# we need to turn on cross-validation for the RFCDE learner
lrn_cv_rfcde <- Lrnr_cv$new(
  learner = lrn_rfcde,
  full_fit = TRUE
)

# modify learner list, using existing SL for Q fit
learner_list <- list(Y = sl_lrn, A = lrn_cv_rfcde)
```

Having made the above preparations, we're now ready to estimate the
counterfactual mean of the weight-for-height Z-score under a small grid of
shifts in the mother's age at child's birth. Just as before, we do this through
a simple call to our `tmle3` wrapper function:

```{r fit_tmle_wrapper_washb_shift, message=FALSE, warning=FALSE, eval=FALSE}
washb_tmle_fit <- tmle3(washb_vim_spec, washb_data, node_list, learner_list)
washb_tmle_fit
```

---

## Exercises

### The Ideas in Action

1. Set the `sl3` library of algorithms for the Super Learner to a simple,
   interpretable library and use this new library to estimate the counterfactual
   mean of mother's age at child's birth (`momage`) under a shift $\delta = 0$.
   What does this counterfactual mean equate to in terms of the observed data?

2. Using a grid of values of the shift parameter $\delta$ (e.g., $\{-1, 0,
   +1\}$), repeat the analysis on the variable chosen in the preceding question,
   summarizing the trend for this sequence of shifts using a marginal structural
   model.

3. Repeat the preceding analysis, using the same grid of shifts, but instead
   directly targeting the parameters of the marginal structural model. Interpret
   the results -- that is, what does the slope of the marginal structural model
   tell us about the trend across the chosen sequence of shifts?

### Review of Key Concepts

1. Describe two (equivalent) ways in which the causal effects of stochastic
   interventions may be interpreted.

2. How does the marginal structural model we used to summarize the trend along
   the sequence of shifts previously help to contextualize the estimated effect
   for a single shift? That is, how does access to estimates across several
   shifts and the marginal structural model parameters allow us to more richly
   interpret our findings?

3. What advantages, if any, are there to targeting directly the parameters of a
   marginal structural model?

<!--
- @haneuse2013estimation characterization of stochastic interventions as
  \textit{modified treatment policies} (MTPs).
- Assumption of \textit{piecewise smooth invertibility} allows for the
  intervention distribution of any MTP to be recovered:
  \begin{equation*}
    g_{0, \delta}(a \mid w) = \sum_{j = 1}^{J(w)} I_{\delta, j} \{h_j(a, w),
    w\} g_0\{h_j(a, w) \mid w\} h^{\prime}_j(a,w)
  \end{equation*}
- Such intervention policies account for the natural value of the
  intervention $A$ directly yet are interpretable as the imposition of an
  altered intervention mechanism.
- Piecewise smooth invertibility: This assumption ensures that we can
  use the change of variable formula when computing integrals over $A$ and
  it is useful to study the estimators that we propose in this paper.

- __Asymptotic linearity:__
  \begin{equation*}
    \Psi(P_n^{\star}) - \Psi(P_0) = \frac{1}{n} \sum_{i = 1}^{n} D(P_0)(X_i) +
    o_P\left(\frac{1}{\sqrt{n}}\right)
  \end{equation*}
- Gaussian limiting distribution:
  \begin{equation*}
    \sqrt{n}(\Psi(P_n^{\star}) - \Psi(P_0)) \to N(0, Var(D(P_0)(O)))
  \end{equation*}
- Statistical inference:
  \begin{equation*}
    \text{Wald-type CI}: \Psi(P_n^{\star}) \pm z_{\alpha} \cdot
    \frac{\sigma_n}{\sqrt{n}},
  \end{equation*}
  where $\sigma_n^2$ is computed directly via
  $\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\cdot)(O_i)$.

Under the additional condition that the remainder term $R(\hat{P}^*, P_0)$
decays as $o_P \left( \frac{1}{\sqrt{n}} \right),$ we have that
$\Psi_n - \Psi_0 = (P_n - P_0) \cdot D(P_0) + o_P
\left( \frac{1}{\sqrt{n}} \right),$ which, by a central limit theorem,
establishes a Gaussian limiting distribution for the estimator, with variance
$V(D(P_0))$, the variance of the efficient influence function
when $\Psi$ admits an asymptotically linear representation.

The above implies that $\Psi_n$ is a $\sqrt{n}$-consistent estimator of $\Psi$,
that it is asymptotically normal (as given above), and that it is locally
efficient. This allows us to build Wald-type confidence intervals, where
$\sigma_n^2$ is an estimator of $V(D(P_0))$. The estimator $\sigma_n^2$
may be obtained using the bootstrap or computed directly via
$\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^*, g_n)(O_i)$

We obtain semiparametric-efficient estimation and robust inference in the
nonparametric model $\M$ by solving the efficient influence function.

1. If $D(\bar{Q}_n^*, g_n)$ converges to $D(P_0)$ in $L_2(P_0)$ norm.
2. The size of the class of functions $\bar{Q}_n^*$ and $g_n$ is bounded
   (technically, $\exists \mathcal{F}$ st
   $D(\bar{Q}_n^*, g_n) \in \mathcal{F}$ whp, where $\mathcal{F}$ is a
   Donsker class)
-->

<!--chapter:end:08-tmle3shift.Rmd-->

# A Primer on the `R6` Class System {#r6}

A central goal of the Targeted Learning statistical paradigm is to estimate
scientifically relevant parameters in realistic (usually nonparametric) models.

The `tlverse` is designed using basic OOP principles and the `R6` OOP framework.
While we've tried to make it easy to use the `tlverse` packages without worrying
much about OOP, it is helpful to have some intuition about how the `tlverse` is
structured. Here, we briefly outline some key concepts from OOP. Readers
familiar with OOP basics are invited to skip this section.

## Classes, Fields, and Methods

The key concept of OOP is that of an object, a collection of data and functions
that corresponds to some conceptual unit. Objects have two main types of
elements:

1. _fields_, which can be thought of as nouns, are information about an object,
   and
2. _methods_, which can be thought of as verbs, are actions an object can
   perform.

Objects are members of classes, which define what those specific fields and
methods are. Classes can inherit elements from other classes (sometimes called
base classes) -- accordingly, classes that are similar, but not exactly the
same, can share some parts of their definitions.

Many different implementations of OOP exist, with variations in how these
concepts are implemented and used. R has several different implementations,
including `S3`, `S4`, reference classes, and `R6`. The `tlverse` uses the `R6`
implementation. In `R6`, methods and fields of a class object are accessed using
the `$` operator. For a more thorough introduction to `R`'s various OOP systems,
see http://adv-r.had.co.nz/OO-essentials.html, from Hadley Wickham's _Advanced
R_ [@wickham2014advanced].

## Object Oriented Programming: `Python` and `R`

OO concepts (classes with inherentence) were baked into Python from the first
published version (version 0.9 in 1991). In contrast, `R` gets its OO "approach"
from its predecessor, `S`, first released in 1976. For the first 15 years, `S`
had no support for classes, then, suddenly, `S` got two OO frameworks bolted on
in rapid succession: informal classes with `S3` in 1991, and formal classes with
`S4` in 1998. This process continues, with new OO frameworks being periodically
released, to try to improve the lackluster OO support in `R`, with reference
classes (`R5`, 2010) and `R6` (2014). Of these, `R6` behaves most like Python
classes (and also most like OOP focused languages like C++ and Java), including
having method definitions be part of class definitions, and allowing objects to
be modified by reference.

<!--chapter:end:98-r6_primer.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:99-references.Rmd-->

